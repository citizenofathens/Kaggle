{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# kaggle 사용법\n",
    "# If any one still need it. if you are using Linux:\n",
    "# 1.pip install kaggle\n",
    "# 2.cd ~/.kaggle\n",
    "# 3.homepage www.kaggle.com -> Your Account -> Create New API token\n",
    "# 4.mv ~/Downloads/kaggle.json ./\n",
    "# 5.chmod 600 ./kaggle.json\n",
    "#\n",
    "# This was answered originally here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModel , AutoConfig,\n",
    "    AutoTokenizer , logging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "import gc ; gc.enable()\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "train = pd.read_csv('../input/commonlitreadabilityprize/train.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooler Output Shape: (16, 768) \n",
      "Logits Shape : (16, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "28617"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.set_verbosity_error()\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "\n",
    "train_text = train['excerpt'][:16].tolist()\n",
    "max_seq_length = 256\n",
    "_pretrained_model = 'roberta-base'\n",
    "\n",
    "config = AutoConfig.from_pretrained(_pretrained_model)\n",
    "model = AutoModel.from_pretrained(_pretrained_model , config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "features  = tokenizer.batch_encode_plus(\n",
    "    train_text,\n",
    "    add_special_tokens=True,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(features['input_ids'] , features['attention_mask'])\n",
    "pooler_output = outputs[1]\n",
    "logits = nn.Linear(config.hidden_size, 1 ) ( pooler_output) # regression head\n",
    "\n",
    "\n",
    "print(f\"Pooler Output Shape: {pooler_output.detach().numpy().shape } \" )\n",
    "print(f'Logits Shape : {logits.detach().numpy().shape}')\n",
    "\n",
    "del config, model, tokenizer , features ,   outputs\n",
    "gc.collect()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModel , AutoConfig,\n",
    "    AutoTokenizer, logging\n",
    ")\n",
    "logging.set_verbosity_error()\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "\n",
    "train_text  = train['excerpt'][:16].tolist()\n",
    "max_seq_length = 256\n",
    "_pretrained_model = 'roberta-base'\n",
    "\n",
    "config = AutoConfig.from_pretrained(_pretrained_model)\n",
    "model = AutoModel.from_pretrained(_pretrained_model, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n",
    "\n",
    "\n",
    "clear_output()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "features = tokenizer.batch_encode_plus(\n",
    "    train_text,\n",
    "    add_special_tokens=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_seq_length,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    return_attention_mask=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(features['input_ids'], features['attention_mask'])\n",
    "last_hidden_state = outputs[0]\n",
    "attention_mask = features['attention_mask']\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 256, 768])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Hidden State Output Shape:  (16, 256, 768)\n",
      "Mean Embeddings Output Shape : (16, 768)\n",
      "Logits Shape : (16, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "112133"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "\n",
    "sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded , 1 )\n",
    "sum_mask = input_mask_expanded.sum(1)\n",
    "sum_mask = torch.clamp(sum_mask , min=1e-9)\n",
    "mean_embeddings = sum_embeddings / sum_mask\n",
    "logits =  nn.Linear(config.hidden_size, 1)(mean_embeddings) # regression head\n",
    "\n",
    "print(f\"Last Hidden State Output Shape:  {last_hidden_state.detach().numpy().shape}\")\n",
    "print(f\"Mean Embeddings Output Shape : {mean_embeddings.detach().numpy().shape}\")\n",
    "print(f\"Logits Shape : {logits.detach().numpy().shape}\")\n",
    "\n",
    "del config, model, tokenizer, features\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModel , AutoConfig,\n",
    "AutoTokenizer, logging\n",
    ")\n",
    "\n",
    "logging.set_verbosity_warning()\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "train_text = train['excerpt'][:16].tolist()\n",
    "max_seq_length = 256\n",
    "_pretrained_model = 'roberta-base'\n",
    "\n",
    "\n",
    "config  = AutoConfig.from_pretrained(_pretrained_model)\n",
    "model = AutoModel.from_pretrained(_pretrained_model, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "features = tokenizer.batch_encode_plus(\n",
    "    train_text,\n",
    "    add_special_tokens=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_seq_length,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(features['input_ids'], features['attention_mask'])\n",
    "\n",
    "last_hidden_state = outputs[0]\n",
    "attention_mask = features['attention_mask']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 256, 768])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.LongTensor{[16, 256, 1]}, size=[3]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [29]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpand\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: expand(torch.LongTensor{[16, 256, 1]}, size=[3]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    }
   ],
   "source": [
    "#attention_mask.unsqueeze(-1).expand(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Hidden State Output Shape  : (16, 256, 768)\n",
      "Max Embeddings Output Shape  : (16, 768)\n",
      "Logits  Shape  : (16, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "24325"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state[input_mask_expanded == 0] = 1e-9 # Set padding tokens to large nagative value\n",
    "max_embeddings = torch.max(last_hidden_state, 1)[0]\n",
    "\n",
    "logits = nn.Linear(config.hidden_size, 1)(max_embeddings)\n",
    "\n",
    "print(f\"Last Hidden State Output Shape  : {last_hidden_state.detach().numpy().shape}\")\n",
    "print(f\"Max Embeddings Output Shape  : {max_embeddings.detach().numpy().shape}\")\n",
    "print(f\"Logits  Shape  : {logits.detach().numpy().shape}\")\n",
    "\n",
    "\n",
    "del config, model,  tokenizer, features\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModel , AutoConfig,\n",
    "AutoTokenizer, logging\n",
    ")\n",
    "\n",
    "logging.set_verbosity_warning()\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "train_text = train['excerpt'][:16].tolist()\n",
    "max_seq_length = 256\n",
    "_pretrained_model = 'roberta-base'\n",
    "\n",
    "\n",
    "config  = AutoConfig.from_pretrained(_pretrained_model)\n",
    "model = AutoModel.from_pretrained(_pretrained_model, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "features = tokenizer.batch_encode_plus(\n",
    "    train_text,\n",
    "    add_special_tokens=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_seq_length,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(features['input_ids'], features['attention_mask'])\n",
    "\n",
    "last_hidden_state = outputs[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_pooling_embeddings = torch.mean(last_hidden_state, 1)\n",
    "_, max_pooling_embeddings = torch.max(last_hidden_state, 1)\n",
    "mean_max_embeddings = torch.cat((mean_pooling_embeddings, max_pooling_embeddings) , 1)\n",
    "\n",
    "logits = nn.Linear(config.hidden_size*2, 1 )(mean_max_embeddings) # twice the hidden size\n",
    "\n",
    "\n",
    "print(f\"Last Hidden State Output Shape  : {last_hidden_state.detach().numpy().shape}\")\n",
    "print(f\"Max Embeddings Output Shape  : {max_embeddings.detach().numpy().shape}\")\n",
    "print(f\"Logits  Shape  : {logits.detach().numpy().shape}\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModel , AutoConfig,\n",
    "AutoTokenizer, logging\n",
    ")\n",
    "\n",
    "logging.set_verbosity_warning()\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "train_text = train['excerpt'][:16].tolist()\n",
    "max_seq_length = 256\n",
    "_pretrained_model = 'roberta-base'\n",
    "\n",
    "\n",
    "config  = AutoConfig.from_pretrained(_pretrained_model)\n",
    "model = AutoModel.from_pretrained(_pretrained_model, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "features = tokenizer.batch_encode_plus(\n",
    "    train_text,\n",
    "    add_special_tokens=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_seq_length,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(features['input_ids'], features['attention_mask'])\n",
    "\n",
    "last_hidden_state = outputs[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "\n",
    "# first define layers\n",
    "\n",
    "cnn1 = nn.Conv1d(768, 256, kernel_size=2, padding=1)\n",
    "cnn2 = nn.Conv1d(256,1 , kernel_size=2, padding=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 256, 768])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 768, 256])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.permute(0,2,1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Hidden State Output Shape: (16, 768, 256)\n",
      "CNN Embeddings Output Shape: (16, 1, 258)\n",
      "Logits Shape : (16, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "27004"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state = last_hidden_state.permute(0,2,1 )\n",
    "cnn_embeddings = F.relu(cnn1(last_hidden_state))\n",
    "cnn_embeddings = cnn2(cnn_embeddings)\n",
    "logits, _ = torch.max(cnn_embeddings,2)\n",
    "\n",
    "print(f\"Last Hidden State Output Shape: {last_hidden_state.detach().numpy().shape}\")\n",
    "print(f\"CNN Embeddings Output Shape: {cnn_embeddings.detach().numpy().shape }\")\n",
    "print(f\"Logits Shape : {logits.detach().numpy().shape }\")\n",
    "\n",
    "del config, model, tokenizer, features\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Hidden State Output Shape: (16, 768, 256)\n",
      "CLS Embeddings Output Shape: (16, 768)\n",
      "Logits Shape : (16, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "24606"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModel , AutoConfig,\n",
    "AutoTokenizer, logging\n",
    ")\n",
    "\n",
    "logging.set_verbosity_warning()\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "train_text = train['excerpt'][:16].tolist()\n",
    "max_seq_length = 256\n",
    "_pretrained_model = 'roberta-base'\n",
    "\n",
    "\n",
    "config  = AutoConfig.from_pretrained(_pretrained_model)\n",
    "config.update({'output_hidden_states' : True})\n",
    "model = AutoModel.from_pretrained(_pretrained_model, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "features = tokenizer.batch_encode_plus(\n",
    "    train_text,\n",
    "    add_special_tokens=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_seq_length,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(features['input_ids'], features['attention_mask'])\n",
    "all_hidden_states =  torch.stack(outputs[2])\n",
    "\n",
    "layer_index = 11 # second to last hidden layer\n",
    "cls_embeddings = all_hidden_states[layer_index+1, :, 0 ] # layer_index + 1 as we have 13 layers (embeddings + num of blocks )\n",
    "\n",
    "\n",
    "logits = nn.Linear(config.hidden_size , 1 )(cls_embeddings) # regression head\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Last Hidden State Output Shape: {last_hidden_state.detach().numpy().shape}\")\n",
    "print(f\"CLS Embeddings Output Shape: {cls_embeddings.detach().numpy().shape }\")\n",
    "print(f\"Logits Shape : {logits.detach().numpy().shape }\")\n",
    "\n",
    "del config, model, tokenizer, features\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModel , AutoConfig,\n",
    "AutoTokenizer, logging\n",
    ")\n",
    "\n",
    "logging.set_verbosity_warning()\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "train_text = train['excerpt'][:16].tolist()\n",
    "max_seq_length = 256\n",
    "_pretrained_model = 'roberta-base'\n",
    "\n",
    "\n",
    "config  = AutoConfig.from_pretrained(_pretrained_model)\n",
    "config.update({'output_hidden_states' : True})\n",
    "model = AutoModel.from_pretrained(_pretrained_model, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "features = tokenizer.batch_encode_plus(\n",
    "    train_text,\n",
    "    add_special_tokens=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_seq_length,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(features['input_ids'], features['attention_mask'])\n",
    "all_hidden_states =  torch.stack(outputs[2])\n",
    "\n",
    "\n",
    "concatenate_pooling = torch.cat(\n",
    "    (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hidden State Output Shape: (13, 16, 256, 768)\n",
      "Concatenate Pooling Output Shape: (16, 3072)\n",
      "Logits Shape : (16, 1)\n"
     ]
    }
   ],
   "source": [
    "concatenate_pooling = concatenate_pooling[:, 0]\n",
    "\n",
    "logits = nn.Linear(config.hidden_size* 4,1)(concatenate_pooling) # regression head\n",
    "\n",
    "\n",
    "print(f\" Hidden State Output Shape: {all_hidden_states.detach().numpy().shape}\")\n",
    "print(f\"Concatenate Pooling Output Shape: {concatenate_pooling.detach().numpy().shape }\")\n",
    "print(f\"Logits Shape : {logits.detach().numpy().shape }\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "del config , model , tokenizer ,features\n",
    "gc.collect();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModel , AutoConfig,\n",
    "AutoTokenizer, logging\n",
    ")\n",
    "\n",
    "logging.set_verbosity_warning()\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "train_text = train['excerpt'][:16].tolist()\n",
    "max_seq_length = 256\n",
    "_pretrained_model = 'roberta-base'\n",
    "\n",
    "\n",
    "config  = AutoConfig.from_pretrained(_pretrained_model)\n",
    "config.update({'output_hidden_states' : True})\n",
    "model = AutoModel.from_pretrained(_pretrained_model, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "features = tokenizer.batch_encode_plus(\n",
    "    train_text,\n",
    "    add_special_tokens=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_seq_length,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(features['input_ids'], features['attention_mask'])\n",
    "all_hidden_states =  torch.stack(outputs[2])\n",
    "\n",
    "\n",
    "concatenate_pooling = torch.cat(\n",
    "    (all_hidden_states [-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hidden State Output Shape: (13, 16, 256, 768)\n",
      "Concatenate Pooling Output Shape: (16, 768)\n",
      "Logits Shape : (16, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "61069"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers,  layer_start : int= 4 , layer_weights = None):\n",
    "\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "            torch.tensor([1] * (num_hidden_layers+1 - layer_start) , dtype=torch.float)\n",
    "\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start: , :, :,:]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average\n",
    "\n",
    "layer_start = 0\n",
    "pooler = WeightedLayerPooling(\n",
    "    config.num_hidden_layers,\n",
    "    layer_start = layer_start, layer_weights=None\n",
    ")\n",
    "\n",
    "\n",
    "weighted_pooling_embeddings = pooler(all_hidden_states)\n",
    "weighted_pooling_embeddings = weighted_pooling_embeddings[: , 0]\n",
    "logits = nn.Linear(config.hidden_size , 1)(weighted_pooling_embeddings)\n",
    "\n",
    "print(f\" Hidden State Output Shape: {all_hidden_states.detach().numpy().shape}\")\n",
    "print(f\"Concatenate Pooling Output Shape: {weighted_pooling_embeddings.detach().numpy().shape }\")\n",
    "print(f\"Logits Shape : {logits.detach().numpy().shape }\")\n",
    "\n",
    "del config, model, tokenizer, features\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([13])"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooler.layer_weights.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooler.layer_start"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "WeightedLayerPooling()"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModel, AutoConfig,\n",
    "    AutoTokenizer, logging\n",
    ")\n",
    "logging.set_verbosity_error()\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "train_text = train['excerpt'][:16].tolist()\n",
    "max_seq_length = 256\n",
    "_pretrained_model = 'roberta-base'\n",
    "\n",
    "config = AutoConfig.from_pretrained(_pretrained_model)\n",
    "config.update({'output_hidden_states':True})\n",
    "model = AutoModel.from_pretrained(_pretrained_model, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "features = tokenizer.batch_encode_plus(\n",
    "    train_text,\n",
    "    add_special_tokens=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_seq_length,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(features['input_ids'], features['attention_mask'])\n",
    "# outputs[2] == outputs.hidden_states, # 맨 위에 특정 헤드 없이 원시 은닉 상태를 출력하는 베어 RoBERTa 모델 변환,마지막 은닉 상태(출력) 이 아닌 모든 은닉 상태를 가져온다\n",
    "all_hidden_states = torch.stack(outputs[2])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "## lstm network와 intermediate representation of the [CLS] Token 을 연결한다 (x)\n",
    "#LSTM 네트워크를 사용하여 [CLS] 토큰의 모든 중간 표현을 연결  (o)\n",
    "class LSTMPooling(nn.Module):\n",
    "    def __init__(self, num_layers , hidden_size , hiddendim_lstm):\n",
    "        super(LSTMPooling, self).__init__()\n",
    "        self.num_hidden_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hiddendim_lstm = hiddendim_lstm\n",
    "        self.lstm = nn.LSTM(self.hidden_size , self.hiddendim_lstm, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        ## forward\n",
    "        hidden_states =  torch.stack([all_hidden_states[layer_i][:, 0].squeeze() for layer_i in range(1, self.num_hidden_layers+1)],dim=-1)\n",
    "        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n",
    "\n",
    "\n",
    "        out, _ = self.lstm(hidden_states, None)\n",
    "        out = self.dropout(out[:,-1,:])\n",
    "        return out\n",
    "hiddendim_lstm = 256\n",
    "pooler = LSTMPooling(config.num_hidden_layers, config.hidden_size , hiddendim_lstm)\n",
    "#\n",
    "lstm_pooling_embeddings = pooler(all_hidden_states)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 768])"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_layer,  batch_size ,max_len, hidden_size\n",
    "all_hidden_states[0][:,0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 256, 768])"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_layer,  batch_size ,max_len, hidden_size\n",
    "all_hidden_states[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 768])"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_layer,  batch_size ,max_len, hidden_size\n",
    "all_hidden_states[0][:,0].squeeze().shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 768, 12])"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size(하나의 글,처리 단위), hidden_size cls token만 가져오는 코드\n",
    "hidden_states = torch.stack([all_hidden_states[layer_i][:,0].squeeze() for layer_i in range(1, config.num_hidden_layers+1)],dim=-1)\n",
    "hidden_states.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "hidden_states = hidden_states.view(-1, config.num_hidden_layers, config.hidden_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 12, 768])"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hidden State Output Shape: (13, 16, 256, 768)\n",
      "Concatenate Pooling Output Shape: (16, 256)\n",
      "Logits Shape : (16, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "102762"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(f\" Hidden State Output Shape: {all_hidden_states.detach().numpy().shape}\")\n",
    "print(f\"Concatenate Pooling Output Shape: {lstm_pooling_embeddings.detach().numpy().shape }\")\n",
    "print(f\"Logits Shape : {logits.detach().numpy().shape }\")\n",
    "\n",
    "\n",
    "del config , model, tokenizer ,  features\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModel, AutoConfig,\n",
    "    AutoTokenizer, logging\n",
    ")\n",
    "logging.set_verbosity_error()\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "train_text = train['excerpt'][:16].tolist()\n",
    "max_seq_length = 256\n",
    "_pretrained_model = 'roberta-base'\n",
    "\n",
    "config = AutoConfig.from_pretrained(_pretrained_model)\n",
    "config.update({'output_hidden_states':True})\n",
    "model = AutoModel.from_pretrained(_pretrained_model, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "features = tokenizer.batch_encode_plus(\n",
    "    train_text,\n",
    "    add_special_tokens=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_seq_length,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(features['input_ids'], features['attention_mask'])\n",
    "# outputs[2] == outputs.hidden_states, # 맨 위에 특정 헤드 없이 원시 은닉 상태를 출력하는 베어 RoBERTa 모델 변환,마지막 은닉 상태(출력) 이 아닌 모든 은닉 상태를 가져온다\n",
    "all_hidden_states = torch.stack(outputs[2])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([13, 16, 256, 768])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "all_hidden_states.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([13, 16, 768, 256])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_len 과 embeddnig 사이즈를 transpose\n",
    "all_hidden_states.transpose(-2,-1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 768])"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hidden_states[0][:,0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 768])"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hidden_states[0][:,0].squeeze().shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "12"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_hidden_layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# stack을 dim=-1 마지막 차원에 쌓는다\n",
    "hidden_states = torch.stack([all_hidden_states[layer_i][:,0].squeeze() for layer_i in range(1,config.num_hidden_layers+1)],dim=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 768, 12])"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([all_hidden_states[layer_i][:,0].squeeze() for layer_i in range(1,config.num_hidden_layers+1)],dim=-1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 12, 768])"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 레이어를 순회하면서 cls token 값만을 stack하면서 쌓는다"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# reshape 한다 tensor를\n",
    "hidden_states = hidden_states.view(-1, config.num_hidden_layers, config.hidden_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 12, 768])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# max_len 과 embeddnig 사이즈를 transpose\n",
    "# query parameter와 matmul\n",
    "import numpy as np\n",
    "q_t = np.random.normal(loc=0.0, scale=0.1, size=(1, config.hidden_size))\n",
    "q = nn.Parameter(torch.from_numpy(q_t)).float()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 768])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 12, 768])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 768, 12])"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.transpose(-2,-1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 1, 12])"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query와 matmul 하여 하나의 값으로 만들도록 matmul\n",
    "torch.matmul(q,hidden_states.transpose(-2,-1)).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "v = torch.matmul(q,hidden_states.transpose(-2,-1)).squeeze(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 12])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "# 문장 max_len 에서 마지막 차원에 softmax화\n",
    "v_ = F.softmax(v,-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 12])"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 1, 768])"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(v_.unsqueeze(1), hidden_states).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 1, 768])"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(v_.unsqueeze(1), hidden_states).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "v_temp = torch.matmul(v_.unsqueeze(1), hidden_states).transpose(-2,-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 768, 1])"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_temp.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "hiddendim_fc = 128"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "w_ht = np.random.normal(loc=0.0, scale=0.1, size=(config.hidden_size,hiddendim_fc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "w_h = nn.Parameter(torch.from_numpy(w_ht)).float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([768, 128])"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_h.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 128, 1])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(w_h.transpose(1,0), v_temp\n",
    ").shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "final_v = torch.matmul(w_h.transpose(1,0), v_temp\n",
    ").squeeze(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 128])"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_v.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "['When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landscape.\\nThe floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. The numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. Also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches.\\nAt each end of the room, on the wall, hung a beautiful bear-skin rug.\\nThese rugs were for prizes, one for the girls and one for the boys. And this was the game.\\nThe girls were gathered at one end of the room and the boys at the other, and one end was called the North Pole, and the other the South Pole. Each player was given a small flag which they were to plant on reaching the Pole.\\nThis would have been an easy matter, but each traveller was obliged to wear snowshoes.',\n 'All through dinner time, Mrs. Fayre was somewhat silent, her eyes resting on Dolly with a wistful, uncertain expression. She wanted to give the child the pleasure she craved, but she had hard work to bring herself to the point of overcoming her own objections.\\nAt last, however, when the meal was nearly over, she smiled at her little daughter, and said, \"All right, Dolly, you may go.\"\\n\"Oh, mother!\" Dolly cried, overwhelmed with sudden delight. \"Really?\\nOh, I am so glad! Are you sure you\\'re willing?\"\\n\"I\\'ve persuaded myself to be willing, against my will,\" returned Mrs. Fayre, whimsically. \"I confess I just hate to have you go, but I can\\'t bear to deprive you of the pleasure trip. And, as you say, it would also keep Dotty at home, and so, altogether, I think I shall have to give in.\"\\n\"Oh, you angel mother! You blessed lady! How good you are!\" And Dolly flew around the table and gave her mother a hug that nearly suffocated her.',\n 'As Roger had predicted, the snow departed as quickly as it came, and two days after their sleigh ride there was scarcely a vestige of white on the ground. Tennis was again possible and a great game was in progress on the court at Pine Laurel. Patty and Roger were playing against Elise and Sam Blaney, and the pairs were well matched.\\nBut the long-contested victory finally went against Patty, and she laughingly accepted defeat.\\n\"Only because Patty\\'s not quite back on her game yet,\" Roger defended; \"this child has been on the sick list, you know, Sam, and she isn\\'t up to her own mark.\"\\n\"Well, I like that!\" cried Patty; \"suppose you bear half the blame, Roger. You see, Mr. Blaney, he is so absorbed in his own Love Game, he can\\'t play with his old-time skill.\"\\n\"All right, Patsy, let it go at that. And it\\'s so, too. I suddenly remembered something Mona told me to tell you, and it affected my service.\"',\n 'And outside before the palace a great garden was walled round, filled full of stately fruit-trees, gray olives and sweet figs, and pomegranates, pears, and apples, which bore the whole year round. For the rich south-west wind fed them, till pear grew ripe on pear, fig on fig, and grape on grape, all the winter and the spring. And at the farther end gay flower-beds bloomed through all seasons of the year; and two fair fountains rose, and ran, one through the garden grounds, and one beneath the palace gate, to water all the town. Such noble gifts the heavens had given to Alcinous the wise.\\nSo they went in, and saw him sitting, like Poseidon, on his throne, with his golden sceptre by him, in garments stiff with gold, and in his hand a sculptured goblet, as he pledged the merchant kings; and beside him stood Arete, his wise and lovely queen, and leaned against a pillar as she spun her golden threads.',\n 'Once upon a time there were Three Bears who lived together in a house of their own in a wood. One of them was a Little, Small, Wee Bear; and one was a Middle-sized Bear, and the other was a Great, Huge Bear. They had each a pot for their porridge; a little pot for the Little, Small, Wee Bear; and a middle-sized pot for the Middle Bear; and a great pot for the Great, Huge Bear. And they had each a chair to sit in; a little chair for the Little, Small, Wee Bear; and a middle-sized chair for the Middle Bear; and a great chair for the Great, Huge Bear. And they had each a bed to sleep in; a little bed for the Little, Small, Wee Bear; and a middle-sized bed for the Middle Bear; and a great bed for the Great, Huge Bear.',\n 'Hal and Chester found ample time to take an inventory of the general\\'s car. It was a huge machine, and besides being fitted up luxuriously was also furnished as an office, that the general might still be at work while he hurried from one part of the field to another when events demanded his immediate presence. Even now, with treachery threatening, and whirling along at a terrific speed, General Joffre, probably because of habit, fell to work sorting papers, studying maps and other drawings.\\nFor almost two hours the car whirled along at top speed, and at length pulled up in the rear of an immense body of troops, who, even to Hal and Chester, could be seen preparing for an advance. General Joffre was out of the car before it came to a full stop, and Hal and Chester were at his heels. An orderly approached.\\n\"My respects to General Tromp, and tell him I desire his presence immediately,\" ordered General Joffre.',\n \"Hal Paine and Chester Crawford were typical American boys. With the former's mother, they had been in Berlin when the great European conflagration broke out and had been stranded there. Mrs. Paine had been able to get out of the country, but Hal and Chester were left behind.\\nIn company with Major Raoul Derevaux, a Frenchman, and Captain Harry Anderson, an Englishman, they finally made their way into Belgium, where they arrived in time to take part in the heroic defense of Liége in the early stages of the war. Here they rendered such invaluable service to the Belgian commander that they were commissioned lieutenants in the little army of King Albert.\\nBoth in fighting and in scouting they had proven their worth. Following the first Belgian campaign, the two lads had seen service with the British troops on the continent, where they were attached to the staff of General Sir John French, in command of the English forces. Also they had won the respect and admiration of General Joffre, the French commander-in-chief.\",\n \"On the twenty-second of February, 1916, an automobile sped northward along the French battle line that for almost two years had held back the armies of the German emperor, strive as they would to win their way farther into the heart of France. For months the opposing forces had battled to a draw from the North Sea to the boundary of Switzerland, until now, as the day waned—it was almost six o'clock—the hands of time drew closer and closer to the hour that was to mark the opening of the most bitter and destructive battle of the war, up to this time.\\nIt was the eve of the battle of Verdun.\\nThe occupants of the automobile as it sped northward numbered three. In the front seat, alone at the driver's wheel, a young man bent low. He was garbed in the uniform of a British lieutenant of cavalry. Close inspection would have revealed the fact that the young man was a youth of some eighteen years, fair and good to look upon.\",\n 'The boys left the capitol and made their way down the long hill to the main business part of the town. As they struck onto the main business street, Garry noticed the familiar blue bell sign of the telephone company.\\n\"Say, boys, I have an idea. Let\\'s stop in here and put in long distance calls and say hello to our folks. How does the idea strike you?\" said Garry, almost in one breath.\\n\"Ripping,\" shouted Phil, while Dick didn\\'t wait to make any remark, but dived in through the door, and in a trice was putting in his call. Phil followed suit, while Garry waited, as he would talk when Dick had finished.\\nThis pleasant duty done, they went to a restaurant for dinner. Here they attracted no little attention, for their khaki clothes looked almost like uniforms. Added to this was the fact that they wore forest shoepacks, those high laced moccasins with an extra leather sole, and felt campaign hats.',\n 'One day he had gone beyond any point which he had ever before visited. He traveled through an open wood, which enabled him to see a great distance. At length he beheld a light breaking through the foliage of the distant trees, which made him sure that he was on the borders of a prairie. It was a wide plain, covered with long blue grass, and enameled with flowers of a thousand lovely tints.\\nAfter walking for some time without a path, musing upon the open country, and enjoying the fragrant breeze, he suddenly came to a ring worn among the grass and the flowers, as if it had been made by footsteps moving lightly round and round. But it was strange—so strange as to cause the White Hawk to pause and gaze long and fixedly upon the ground—there was no path which led to this flowery circle. There was not even a crushed leaf nor a broken twig, nor the least trace of a footstep, approaching or retiring, to be found. He thought he would hide himself and lie in wait to discover, if he could, what this strange circle meant.',\n \"It was believed by the principal men of Virginia that Talbot's sympathies were with the revolted colonies; but the influence of his mother, to whom he had been accustomed to defer, had hitherto proved sufficient to prevent him from openly declaring himself. His visit to England, and the delightful reception he had met with there, had weakened somewhat the ties which bound him to his native country, and he found himself in a state of indecision as humiliating as it was painful. Lord Dunmore and Colonel Wilton had each made great efforts to enlist his support, on account of his wealth and position and high personal qualities. It was hinted by one that the ancient barony of the Talbots would be revived by the king; and the gratitude of a free and grateful country, with the consciousness of having materially aided in acquiring that independence which should be the birthright of every Englishman, was eloquently portrayed by the other. When to the last plea was added the personal preference of Katharine Wilton, the balance was overcome, and the hopes of the mother were doomed to disappointment.\",\n 'This Pedrarias was seventy-two years old. He was of good birth and rich, and was the father of a large and interesting family, which he prudently left behind him in Spain. His wife, however, insisted on going with him to the New World. Whether or not this was a proof of wifely devotion—and if it was, it is the only thing in history to his credit—or of an unwillingness to trust Pedrarias out of her sight, which is more likely, is not known. At any rate, she went along.\\nPedrarias, up to the time of his departure from Spain, had enjoyed two nick-names, El Galan and El Justador. He had been a bold and dashing cavalier in his youth, a famous tilter in tournaments in his middle age, and a hard-fighting soldier all his life. His patron was Bishop Fonseca. Whatever qualities he might possess for the important work about to be devolved upon him would be developed later.',\n 'The Emperor walked nervously up and down the long, low-ceiled apartment, the common room of the public inn at Nogent. Grouped around a long table in the center of the room several secretaries were busy with orders, reports and dispatches. At one end stood a group of officers of high rank in rich uniforms whose brilliance was shrouded by heavy cloaks falling from their shoulders and gathered about them, for the air was raw and chill, despite a great fire burning in a huge open fireplace. Their cloaks and hats were wet, their boots and trousers splashed with mud, and in general they were travel-stained and weary. They eyed the Emperor, passing and repassing, in gloomy silence mixed with awe. In their bearing no less than in their faces was expressed a certain unwonted fierce resentment, which flamed up and became more evident when the Emperor turned his back in his short, restless march to and fro, but which subsided as suddenly when he had them under observation. By the door was stationed a young officer in the uniform of the Fifth Regiment of the infantry of the line.',\n 'The clock in a nearby church struck the hour of two. The areaway was dark. No one was abroad. He plunged down the steps, opened the window and disappeared. No man could move more noiselessly than he. In the still night he knew how the slightest sounds are magnified. He had made none as he groped his way through the back of the house, arriving at last in a room which he judged to be the library. Then, after listening and hearing nothing, he ventured to turn the button of a side light in a far corner of the room.\\nHe was in a large apartment, beautifully furnished. Books and pictures abounded, but these did not interest him, although if he had made further examination, he might have found things worthy of his attention even there. It so happened that the light bracket to which he had blundered, or had been led, was immediately over a large wall safe. Evidently it had been placed there for the purpose of illuminating the safe door. His eyes told him that instantly. This was greater fortune than he expected. A wall safe in a house like that must contain things of value.',\n \"Aunt Abigail was gone, Eleanor was gone. The room was quite empty except for the bright sunshine pouring in through the small-paned windows. Elizabeth Ann stretched and yawned and looked about her. What funny wall-paper it was—so old-fashioned looking! The picture was of a blue river and a brown mill, with green willow-trees over it, and a man with sacks on his horse's back stood in front of the mill. This picture was repeated a great many times, all over the paper; and in the corner, where it hadn't come out even, they had had to cut it right down the middle of the horse. It was very curious-looking. She stared at it a long time, waiting for somebody to tell her when to get up. At home Aunt Frances always told her, and helped her get dressed. But here nobody came. She discovered that the heat came from a hole in the floor near the bed, which opened down into the room below. From it came a warm breath of baking bread and a muffled thump once in a while.\",\n 'So off went Lionel to be made a King. He had never expected to be a King any more than you have, so it was all quite new to him—so new that he had never even thought of it. And as the coach went through the town he had to bite his tongue to be quite sure it was real, because if his tongue was real it showed he wasn\\'t dreaming. Half an hour before he had been building with bricks in the nursery; and now—the streets were all fluttering with flags; every window was crowded with people waving handkerchiefs and scattering flowers; there were scarlet soldiers everywhere along the pavements, and all the bells of all the churches were ringing like mad, and like a great song to the music of their ringing he heard thousands of people shouting, \"Long live Lionel! Long live our little King!\"\\nHe was a little sorry at first that he had not put on his best clothes, but he soon forgot to think about that.']"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size , hiddendim_fc):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.num_hidden_layers = num_layers\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.hiddendim_fc =  hiddendim_fc\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        q_t =  np.random.normal(loc=0.0 , scale=0.1 , size=(1, self.hidden_size))\n",
    "        self.q = nn.Parameter(torch.from_numpy(q_t)).float()\n",
    "        w_ht = np.random.normal(loc=0.0 , scale=0.1 , size=(self.hidden_size , self.hiddendim_fc))\n",
    "\n",
    "        self.w_h = nn.Parameter(torch.from_numpy(w_ht)).float()\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        # cls token을 뽑는 과정\n",
    "        hidden_states =  torch.stack([all_hidden_states[layer_i][: , 0].squeeze() for layer_i in range(1, self.num_hidden_layers)])\n",
    "        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n",
    "\n",
    "        out = self.attention(hidden_states)\n",
    "        out = self.dropout(out )\n",
    "        return out\n",
    "    # o = W_h^T*softmax(q*h^T_cls)h_cls 를 구현한 것이다.\n",
    "    #https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently\n",
    "    def attention(self, h ):\n",
    "        # attention operation은 학습할 수 있다.\n",
    "        # the contribution of each h^i_cls   : (h의 각 i번째 cls 의 분포를, 그래서 attention, 주목해야할 부분을 찾는다 캬.. 실제적으로 생각하면서\n",
    "        # 코드 보고 생각하고 수식 실제적으로 생각하니 이해해냈다)\n",
    "        v = torch.matmul(self.q , h.transpose(-2,-1)).squeeze(1)\n",
    "        v = F.softmax(v, -1)\n",
    "        v_temp = torch.matmul(v.unsqueeze(1), h).transpose(-2,-1 )\n",
    "        v, = torch.matmul(self.w_h.transpose(1,0) , v_temp).squeeze(2)\n",
    "        return v\n",
    "\n",
    "\n",
    "hiddendim_fc = 128\n",
    "pooler = AttentionPooling(config.num_hidden_layers, config.hidden_size, hiddendim_fc)\n",
    "attention_pooling_embeddings = pooler(all_hidden_states)\n",
    "logtis = nn.Linear(hiddendim_fc, 1)(attention_pooling_embeddings)  # regression head\n",
    "\n",
    "# detach : copy\n",
    "print(f\" Hidden State Output Shape: {all_hidden_states.detach().numpy().shape}\")\n",
    "print(f\"Concatenate Pooling Output Shape: {attention_pooling_embeddings.detach().numpy().shape }\")\n",
    "print(f\"Logits Shape : {logits.detach().numpy().shape }\")\n",
    "\n",
    "\n",
    "del config , model , tokenizer,  features\n",
    "gc.collect()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "q_t = np.random.normal(loc=0.0 , scale=0.1 , size=(1, config.hidden_size))\n",
    " # 말 그대로 파라미터만 가지고 있다\n",
    " #%\n",
    "# %"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "q = nn.Parameter(torch.from_numpy(q_t)).float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "hidden_states =  torch.stack([all_hidden_states[layer_i][:, 0].squeeze() for layer_i in range(1, config.num_hidden_layers+1)], dim=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 768, 12])"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "hidden_states = hidden_states.view(-1, config.num_hidden_layers , config.hidden_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 12, 768])"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 768, 12])"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.transpose(-2,-1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 768])"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 12])"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matmul 의 경우 두번째 dimension 과 서로 같아야  계산 가능 matmul은 주어진 차원에 따라 계산 구현이 다름\n",
    "torch.matmul(q, hidden_states.transpose(-2,-1)).squeeze(1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 768])"
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "data": {
      "text/plain": "0.112166"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.50340343e-01,  4.58478509e-02, -7.53045705e-02,\n         8.09586386e-03,  9.93280388e-02,  1.80328811e-02,\n        -5.54247503e-02,  8.95468500e-02,  2.10247239e-02,\n        -2.66704411e-02, -1.18155986e-02, -4.64011779e-02,\n        -1.43496801e-01,  1.11578032e-01,  4.67111154e-03,\n        -8.46342745e-02, -1.12850845e-01,  1.48897500e-01,\n         6.00778453e-02,  1.65535395e-02,  4.46960835e-02,\n         1.39637997e-01,  2.09386843e-01,  2.33615016e-01,\n         1.85463817e-01,  1.51698747e-01, -4.09955507e-02,\n        -1.34423575e-02, -1.14128348e-01, -2.64406672e-02,\n        -3.27826966e-02, -1.33009834e-01,  3.78011421e-02,\n        -1.19426698e-01, -5.00464016e-02, -7.45591960e-03,\n         6.06261634e-02,  2.09348093e-01, -1.33766525e-01,\n         2.42471421e-01,  1.52789052e-01,  1.24875000e-01,\n         4.75891504e-02, -7.68287647e-02, -8.63998072e-02,\n         7.63915132e-02, -5.87534013e-02,  3.02108096e-02,\n         1.46235568e-01,  1.22103722e-01, -1.62984035e-01,\n        -1.14737931e-02,  5.73045333e-02, -4.78538861e-03,\n         2.39342911e-02, -1.16225515e-01, -3.16116893e-02,\n        -2.16592423e-01,  2.46791793e-02,  1.36356815e-02,\n         4.38178121e-02, -6.99542116e-02,  9.06294744e-02,\n         8.70185203e-02,  6.08118341e-02,  8.30686314e-02,\n        -3.17358380e-02, -4.19429707e-02, -2.03079328e-01,\n        -9.37825643e-02,  2.23120396e-01,  1.04210017e-01,\n         8.67018148e-03, -1.50602961e-01, -1.15409590e-01,\n        -6.66742870e-02, -1.55585033e-01, -1.86292237e-02,\n        -1.36496708e-01,  3.97077107e-02,  1.45943046e-01,\n         1.69932958e-01, -1.54965061e-01,  1.31075575e-02,\n        -3.12009882e-02, -2.64359504e-03, -6.58139920e-02,\n         2.00614498e-01, -2.30304909e-02, -2.11893886e-01,\n         5.70380190e-02,  3.59150841e-02, -9.49893741e-03,\n         1.82318676e-01, -1.79556457e-01, -9.51072258e-02,\n         1.51648232e-01,  9.77149434e-02,  4.09619780e-02,\n         1.45353633e-01,  8.74071896e-03,  5.02513714e-02,\n        -2.88633543e-02,  3.43198083e-02, -7.63417262e-02,\n         1.29127231e-01, -2.44743549e-01,  7.84755721e-02,\n         7.85409428e-02,  8.99456101e-03,  6.86861963e-02,\n        -1.54547778e-02,  3.78369784e-02,  1.72395137e-02,\n         7.32652156e-02, -3.83086564e-02,  7.09313857e-02,\n         1.10499333e-01, -7.79167434e-02, -1.33259419e-01,\n        -2.40130030e-02,  9.01008733e-02,  4.80734489e-02,\n         1.40904627e-01, -2.36206354e-02, -8.40884430e-02,\n         1.06864154e-01, -8.07615596e-02,  4.28670821e-02,\n        -1.38262625e-01, -1.07501618e-01,  1.93368862e-02,\n         7.55946672e-03, -2.35055636e-02, -1.32643972e-01,\n         1.05261243e-01, -2.76677150e-02, -1.46731043e-01,\n         1.23291020e-01, -3.30559359e-02, -1.23400515e-01,\n        -5.44116858e-02, -7.58648393e-02,  1.36693773e-01,\n         1.72024279e-01,  1.76700306e-01,  5.92720658e-02,\n         1.09898153e-01,  7.68857371e-02,  9.20356952e-02,\n        -2.03057128e-01, -5.26230182e-02,  2.03742689e-02,\n         1.74510760e-02, -1.40493703e-01, -2.14047370e-01,\n         1.13431552e-01, -1.08814578e-01, -4.65772361e-02,\n         2.57267786e-02, -4.47004369e-02,  5.00087159e-02,\n        -6.46881489e-02, -7.21489024e-02,  2.73794029e-02,\n         1.16413583e-01, -7.91826363e-02, -1.49261528e-03,\n        -6.28696828e-02,  8.18620809e-02,  1.19877670e-01,\n        -5.13002817e-02,  1.47560249e-01,  6.67337207e-02,\n        -3.71757460e-02, -5.70030523e-02,  5.78881812e-02,\n         3.62680916e-02,  7.88800237e-02, -1.20745625e-03,\n        -2.42760001e-01,  1.95022929e-01, -4.44513560e-02,\n         4.51871526e-02,  1.40161693e-01,  1.48090695e-01,\n        -3.63134524e-02,  1.57219969e-01, -7.86379608e-02,\n        -8.94070268e-02,  3.02091165e-02,  5.74170186e-02,\n         7.75696573e-02, -6.68918610e-02, -7.00638998e-02,\n        -5.40586008e-02,  2.23792674e-02, -1.47042199e-01,\n         2.86225680e-02, -8.47310078e-02, -2.96478061e-03,\n         1.86792754e-01,  1.59577610e-01, -1.25760866e-01,\n        -5.85425891e-02,  3.62773831e-02, -5.33684806e-02,\n         2.17083280e-01,  9.95858434e-02,  3.51116344e-02,\n        -9.64190695e-02, -1.15505898e-01, -1.42412949e-01,\n        -3.86939104e-02,  2.80586644e-01, -3.28407875e-02,\n        -2.37728440e-02, -1.25111496e-01,  3.77597929e-04,\n         1.10170800e-01,  1.33577592e-01,  2.31577294e-02,\n        -3.08708353e-02,  7.59865339e-02,  5.36554600e-02,\n         4.35780363e-02,  6.19227704e-02,  9.59653204e-02,\n         1.95900837e-01,  1.87764717e-02, -3.41359966e-02,\n        -1.72128438e-01,  5.80883914e-02,  2.24978052e-02,\n        -2.08746303e-01, -9.45062210e-03,  9.44028578e-02,\n        -1.64318405e-02, -2.11827282e-02, -4.91382800e-02,\n         1.29086728e-01, -3.75553205e-02, -1.53600495e-02,\n         5.50953209e-03,  1.92230628e-01, -1.05916912e-01,\n        -5.20583609e-02,  5.79504884e-02, -2.64946389e-01,\n        -8.07411932e-02,  7.63690098e-02,  6.23251581e-02,\n        -1.71308796e-01,  1.95158181e-02,  1.03561997e-01,\n        -3.63094530e-02,  7.20947693e-02, -3.78729410e-02,\n        -8.11489665e-02, -5.15228776e-02, -1.85691318e-01,\n        -5.12361253e-02,  6.18060149e-02, -1.00666087e-02,\n        -2.32641622e-04, -1.62683002e-03,  1.84892080e-01,\n         1.27979887e-01,  7.63854696e-02,  2.63602242e-01,\n        -1.27355806e-01, -5.49134907e-02, -5.55171414e-02,\n        -1.28409857e-01, -9.27140068e-02, -6.86721701e-02,\n        -1.43632651e-01,  7.67306043e-02, -2.72152695e-02,\n         3.85809510e-03, -2.50721143e-02,  6.99414219e-02,\n         1.13850401e-01, -5.92960679e-02, -1.73321565e-02,\n         9.97004254e-02,  1.71768009e-01, -3.47565804e-02,\n         1.12034744e-01,  1.25551586e-01,  5.25447580e-02,\n         1.35225402e-01, -1.59873241e-02,  3.05563004e-02,\n        -1.48390303e-01,  1.86227905e-01,  1.46772860e-01,\n         2.30445202e-02,  2.74469232e-02,  3.83149074e-03,\n        -7.10644283e-02,  1.17166903e-02,  6.00146605e-02,\n         9.70475460e-02, -6.44404309e-02, -3.62224737e-02,\n        -2.01335809e-01, -5.19901122e-02, -6.64368590e-02,\n        -1.17647841e-02,  5.47903053e-02, -1.21697472e-02,\n        -3.39195098e-02, -8.00122183e-02, -1.76589215e-01,\n        -1.85110750e-01, -1.38630451e-02, -8.20804974e-02,\n        -1.40315365e-01, -5.49032728e-02, -3.88161198e-02,\n        -5.83726961e-02,  1.56976964e-01, -5.86802242e-02,\n        -6.67548800e-02, -2.93500198e-02,  8.39270093e-02,\n         7.91971196e-02, -1.11861288e-04,  2.47735896e-01,\n        -8.54095240e-02, -6.39919331e-02, -5.60804393e-02,\n         1.29337924e-01, -6.54895889e-02,  1.98971113e-02,\n        -1.76574762e-01, -3.14490246e-03,  3.18077494e-02,\n         1.35047326e-01,  6.75622576e-02, -6.38818597e-02,\n        -8.07726965e-02, -1.46043351e-02,  2.37779521e-01,\n        -3.74512936e-02,  8.05519594e-02, -5.93513385e-02,\n         7.98410908e-02, -1.94839529e-02, -1.08830242e-01,\n        -6.55517166e-02,  4.51046700e-02, -1.55232726e-02,\n         6.98933724e-02,  8.77566767e-02,  3.81608262e-02,\n         2.70657587e-01,  8.57962643e-02, -2.15823703e-02,\n        -8.86857699e-02,  8.65949629e-02, -6.64489518e-02,\n        -2.16772541e-02, -1.12743204e-01, -9.57331411e-02,\n         2.01519992e-01, -2.07785832e-01, -1.44148120e-01,\n         1.45209279e-01, -4.38820610e-02, -7.49082721e-02,\n        -9.41936099e-02,  9.60942567e-02, -1.36153033e-01,\n        -7.33430037e-02, -7.88179235e-02,  2.02882737e-02,\n        -2.50375354e-01,  7.56913487e-02,  1.84065608e-02,\n        -1.54470528e-01,  1.53501243e-01, -1.21181352e-01,\n        -5.62952068e-02, -7.31369540e-02, -5.63816201e-02,\n         2.01153749e-02,  8.12111216e-02,  9.32918063e-02,\n        -1.73292112e-01, -6.76715167e-02,  8.57747742e-02,\n         1.38447993e-01,  1.46208593e-01,  2.00706784e-01,\n        -1.53395166e-01, -7.02133808e-02, -2.03570615e-01,\n        -2.33821076e-02,  1.03454953e-01, -8.42212129e-02,\n        -1.43445383e-01, -3.75115971e-02,  1.07754655e-01,\n        -1.23586998e-01, -1.68345985e-02, -3.67811546e-02,\n         8.07701386e-02,  1.26711794e-01, -9.98548591e-02,\n        -3.37855785e-02, -1.50169529e-01,  2.76672011e-03,\n         5.07733334e-02, -2.71015793e-02,  1.21208492e-01,\n         9.53951047e-02,  1.04311425e-01, -2.85844955e-02,\n        -1.29672930e-01, -1.49559401e-01, -1.39873618e-01,\n         6.92344014e-02,  2.09480240e-02, -9.97952654e-02,\n        -8.51025814e-02,  1.30703842e-01, -9.31487893e-02,\n        -8.90070368e-02, -1.68724486e-01,  8.32935767e-02,\n         1.41962162e-01, -7.88782832e-02,  6.70111974e-03,\n        -2.33381407e-02,  3.63492052e-02, -8.40615652e-02,\n         3.95558259e-02,  8.16378287e-02, -4.06443213e-02,\n        -1.57986029e-01,  1.01625889e-01, -2.26371849e-01,\n         8.49856678e-02,  6.60758303e-02, -3.83189403e-02,\n         1.18149639e-01, -1.01260641e-01, -6.36596911e-02,\n        -1.58737862e-01, -6.86617656e-02, -1.04874048e-01,\n        -1.54042643e-01,  2.39419012e-02, -4.38793479e-02,\n         6.46111191e-02,  5.57568939e-02,  1.37960717e-01,\n         6.73721549e-02, -1.92432026e-01, -1.52857401e-01,\n         4.24286314e-02,  3.17817552e-02,  4.95811586e-02,\n         9.58957367e-02,  5.04470697e-02, -2.56616740e-02,\n        -5.51554241e-02, -9.18296682e-02, -6.87673468e-02,\n        -2.78689144e-01,  3.64664152e-03,  6.89022096e-02,\n        -2.68438627e-02,  6.99401617e-02, -4.35726650e-03,\n        -2.67168035e-01,  2.85247661e-01,  1.87673381e-02,\n         1.83753710e-01,  5.57881322e-02, -5.30863211e-03,\n        -1.98029513e-02, -3.53131534e-02,  1.16388358e-02,\n         4.23348529e-02,  7.92807702e-03, -1.12955262e-01,\n        -1.68761791e-01,  9.77233940e-02,  5.63745223e-02,\n        -2.42441585e-01,  1.65301758e-01, -3.46799969e-02,\n        -1.78835768e-02, -1.43863316e-01,  7.55332748e-02,\n         1.41440920e-02,  1.26108157e-01, -1.43851828e-01,\n         7.90730097e-02,  2.34496878e-01,  8.46497988e-02,\n         3.05921556e-02, -2.93197474e-02,  1.47786035e-01,\n        -1.36861554e-01,  7.58857738e-02, -6.22808790e-02,\n        -1.85322885e-01, -9.23705260e-02, -3.38411494e-02,\n        -6.57312853e-02, -1.78869078e-02, -1.11465969e-01,\n        -3.25573635e-02,  2.56105100e-01,  3.74094825e-02,\n         1.29680517e-01,  1.31182931e-01,  8.26274003e-02,\n        -1.26542957e-01, -1.85730872e-01, -2.12551247e-01,\n         7.81165015e-02,  6.60842477e-03, -4.38687345e-02,\n        -5.77928828e-02, -1.18651279e-01, -1.18964797e-01,\n        -6.98818256e-03, -2.63771968e-02,  1.23716367e-01,\n        -1.14741365e-01,  7.57077930e-02,  1.41076511e-01,\n         6.64901638e-03, -1.12274775e-01, -7.50197990e-02,\n        -1.12737716e-01,  1.10037672e-01, -6.50898152e-02,\n        -4.78696522e-02,  1.58390588e-01,  7.12349276e-02,\n        -9.34860646e-02,  8.13790538e-02,  6.80287117e-02,\n         7.34119906e-02, -7.77852064e-02, -1.57913972e-01,\n         1.58114300e-01, -3.90838548e-02, -4.40956067e-02,\n         9.15222990e-02,  7.56401308e-02,  3.22473933e-02,\n         2.15773858e-01,  4.81038732e-02,  9.06410757e-02,\n        -8.91142494e-02,  3.07837396e-02, -6.46256359e-02,\n        -2.02938544e-01,  1.66011782e-01,  3.03021085e-02,\n        -4.70293375e-02, -1.21777634e-01,  7.69512954e-02,\n         1.38758727e-01, -5.87941317e-02,  3.22442988e-02,\n        -4.20240144e-02, -1.15114572e-02,  6.42581302e-02,\n         9.14792972e-02, -1.61857140e-01, -3.45991055e-02,\n         1.82391819e-02, -1.07725404e-01, -1.12829512e-01,\n        -1.20171217e-01,  1.15662582e-01,  1.53590227e-01,\n        -2.52825302e-01, -7.04456389e-02, -1.38727094e-01,\n         1.39540128e-01, -7.97908829e-02, -7.98259459e-02,\n        -2.72836490e-02,  5.47745074e-02, -4.15559300e-02,\n         6.03817994e-02,  1.36096318e-01,  1.30731587e-01,\n         4.24946664e-02, -1.47810308e-02,  1.24822776e-01,\n         1.32080817e-01,  8.99199622e-02, -1.18647802e-01,\n        -1.04687953e-01,  2.77160800e-02,  2.78559770e-02,\n         1.94309715e-02,  4.82299632e-02,  6.38898972e-02,\n        -5.74746941e-02, -3.06214682e-02, -1.01990957e-01,\n        -1.74755250e-02, -5.38159790e-02, -1.37093766e-01,\n        -9.13985393e-02, -5.28844562e-02,  2.19910400e-02,\n        -7.71032248e-02, -1.03817626e-01, -1.00290754e-01,\n         1.04504254e-01,  1.90133113e-01,  1.61102373e-01,\n        -6.00193065e-03, -1.34956441e-01, -2.32112451e-02,\n        -4.13149425e-03, -1.08837207e-01, -1.50029053e-01,\n        -1.69509356e-01,  9.50520036e-02,  8.35516320e-02,\n         5.18016210e-02, -1.95713406e-01,  4.47213953e-02,\n        -4.47188731e-02,  1.68647447e-02,  7.37989847e-02,\n        -5.79296126e-02,  2.19306101e-02, -1.11531110e-01,\n        -1.03900475e-01,  3.59746379e-02,  4.28498088e-03,\n        -3.52464217e-02, -5.58628814e-02,  3.52373779e-02,\n         8.77721022e-02, -4.56081817e-02, -6.65713665e-02,\n        -1.48790007e-01,  1.35936539e-02,  3.89392615e-02,\n         4.62911039e-03, -5.16616317e-02,  8.43905754e-02,\n        -1.05176912e-01, -1.00273847e-01,  5.57091587e-02,\n        -1.53922749e-01,  1.64518725e-01,  1.08169276e-01,\n        -7.54270828e-03, -1.82271514e-02, -5.89858820e-02,\n        -3.69440714e-02,  3.60968325e-04,  4.01582794e-02,\n        -5.73505223e-02, -4.90780484e-02, -1.72493005e-01,\n         2.03607348e-03,  6.35683242e-02, -4.85387517e-02,\n        -4.93252352e-02,  1.47457429e-01, -5.36685244e-02,\n         1.36452815e-01, -1.27268247e-01, -1.87078315e-01,\n         5.04397364e-02,  4.27094918e-02, -1.48438784e-01,\n        -2.28960845e-01, -1.01811784e-03,  1.05578181e-01,\n         2.32928615e-02,  5.55722751e-02,  1.20574953e-01,\n        -5.15912044e-02, -3.29012091e-02,  2.15726466e-01,\n         8.65543071e-02,  8.09908307e-03,  3.02763095e-02,\n         2.82413983e-01,  3.03048234e-02, -8.47562024e-02,\n         1.18692110e-01,  1.06478222e-01, -1.14846619e-01,\n         2.50567814e-01,  2.22775922e-01,  4.94649395e-02,\n        -4.34089090e-02,  6.60765384e-02, -6.11808291e-02,\n        -2.32299530e-02,  1.39618827e-01,  1.42253225e-01,\n         2.10713824e-03, -6.80582264e-02, -2.56334716e-01,\n        -9.47935753e-02, -4.05154745e-02, -3.60527645e-02,\n         3.20206284e-02,  7.02325505e-02,  8.16588162e-02,\n        -3.26809303e-02,  1.31602423e-02, -4.57474860e-02,\n        -8.61716268e-02,  8.68221943e-02, -6.81746708e-02,\n        -1.08302836e-01,  6.18800130e-04,  6.01815356e-02,\n        -6.63918184e-02,  2.43860579e-02,  4.95447354e-02,\n        -4.44520262e-02, -7.29487406e-02, -3.17737311e-01,\n         9.62961553e-02, -2.44839091e-01, -1.38863481e-02,\n        -1.85614745e-01, -8.51788097e-02, -6.06271575e-02,\n         2.04932270e-01, -2.88062349e-01,  1.04399784e-01,\n        -9.62566643e-02,  8.89877112e-02,  7.86512309e-02,\n        -4.85654023e-02, -2.28164316e-02,  1.66213597e-01,\n         1.31568238e-02, -6.25061278e-02, -2.26472663e-01,\n         5.09979249e-04, -1.34334956e-01,  3.10600611e-01,\n        -9.59946337e-02, -1.83112471e-01, -4.47032304e-02,\n        -1.42957081e-01, -9.98791796e-02, -1.49115505e-01,\n         6.64989074e-02, -5.86549824e-02,  4.49629475e-02,\n        -7.76832815e-02, -9.10165145e-02,  1.19143079e-01]])"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}