{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "print(sys.path)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "# 상대경로 모르면 절대 경로"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:\\\\Users\\\\tjdal\\\\PycharmProjects\\\\Kaggle\\\\Corona_NLP_train.csv', encoding='latin1')\n",
    "\n",
    "# EDA\n",
    "data.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "originalTweet = data['OriginalTweet']\n",
    "originalTweet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%=\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# unit test\n",
    "stopwords = ['@', 'https://',':']\n",
    "def apply_stopwords(text):\n",
    "    for i in range(len(stopwords)):\n",
    "        if stopwords[i] in text:\n",
    "            text = text.replace(stopwords[i],'')\n",
    "    return text\n",
    "apply_stopwords('@mesdaf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 위의 특수문자 제거\n",
    "data['OriginalTweet'] = data['OriginalTweet'].apply(lambda x: apply_stopwords(x))\n",
    "#preprocessing\n",
    "data['OriginalTweet'] =  data['OriginalTweet'].apply(lambda x: x.split(' '))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "       UserName  ScreenName                      Location     TweetAt  \\\n0          3799       48751                        London  16-03-2020   \n1          3800       48752                            UK  16-03-2020   \n2          3801       48753                     Vagabonds  16-03-2020   \n3          3802       48754                           NaN  16-03-2020   \n4          3803       48755                           NaN  16-03-2020   \n...         ...         ...                           ...         ...   \n41152     44951       89903  Wellington City, New Zealand  14-04-2020   \n41153     44952       89904                           NaN  14-04-2020   \n41154     44953       89905                           NaN  14-04-2020   \n41155     44954       89906                           NaN  14-04-2020   \n41156     44955       89907  i love you so much || he/him  14-04-2020   \n\n                                           OriginalTweet           Sentiment  \n0      [MeNyrbie, Phil_Gahan, Chrisitv, t.co/iFz9FAn2...             Neutral  \n1      [advice, Talk, to, your, neighbours, family, t...            Positive  \n2      [Coronavirus, Australia, Woolworths, to, give,...            Positive  \n3      [My, food, stock, is, not, the, only, one, whi...            Positive  \n4      [Me,, ready, to, go, at, supermarket, during, ...  Extremely Negative  \n...                                                  ...                 ...  \n41152  [Airline, pilots, offering, to, stock, superma...             Neutral  \n41153  [Response, to, complaint, not, provided, citin...  Extremely Negative  \n41154  [You, know, itÂs, getting, tough, when, Kamer...            Positive  \n41155  [Is, it, wrong, that, the, smell, of, hand, sa...             Neutral  \n41156  [TartiiCat, Well, new/used, Rift, S, are, goin...            Negative  \n\n[41157 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3799</td>\n      <td>48751</td>\n      <td>London</td>\n      <td>16-03-2020</td>\n      <td>[MeNyrbie, Phil_Gahan, Chrisitv, t.co/iFz9FAn2...</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3800</td>\n      <td>48752</td>\n      <td>UK</td>\n      <td>16-03-2020</td>\n      <td>[advice, Talk, to, your, neighbours, family, t...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3801</td>\n      <td>48753</td>\n      <td>Vagabonds</td>\n      <td>16-03-2020</td>\n      <td>[Coronavirus, Australia, Woolworths, to, give,...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3802</td>\n      <td>48754</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>[My, food, stock, is, not, the, only, one, whi...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3803</td>\n      <td>48755</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>[Me,, ready, to, go, at, supermarket, during, ...</td>\n      <td>Extremely Negative</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>41152</th>\n      <td>44951</td>\n      <td>89903</td>\n      <td>Wellington City, New Zealand</td>\n      <td>14-04-2020</td>\n      <td>[Airline, pilots, offering, to, stock, superma...</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>41153</th>\n      <td>44952</td>\n      <td>89904</td>\n      <td>NaN</td>\n      <td>14-04-2020</td>\n      <td>[Response, to, complaint, not, provided, citin...</td>\n      <td>Extremely Negative</td>\n    </tr>\n    <tr>\n      <th>41154</th>\n      <td>44953</td>\n      <td>89905</td>\n      <td>NaN</td>\n      <td>14-04-2020</td>\n      <td>[You, know, itÂs, getting, tough, when, Kamer...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>41155</th>\n      <td>44954</td>\n      <td>89906</td>\n      <td>NaN</td>\n      <td>14-04-2020</td>\n      <td>[Is, it, wrong, that, the, smell, of, hand, sa...</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>41156</th>\n      <td>44955</td>\n      <td>89907</td>\n      <td>i love you so much || he/him</td>\n      <td>14-04-2020</td>\n      <td>[TartiiCat, Well, new/used, Rift, S, are, goin...</td>\n      <td>Negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>41157 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 잠재 디리클레 '할당'\n",
    "data\n",
    "nf = 20\n",
    "detokenizng =  data['OriginalTweet'].apply(lambda x: ' '.join(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 행렬 분해 벡터라이즈\n",
    "data\n",
    "\n",
    "data[data['UserName'] == 3799]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# unique user name check\n",
    "print('origin userName :{}'.format(len(data['UserName'])))\n",
    "print('unique userName :{}'.format(len(data['UserName'].unique())))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "l      sss       a          \"\n",
    "l     s         a a         \"\n",
    "l      sss     aaaaa        \"\n",
    "l         s   a     a       \"\n",
    "l___   sss   a       a      \"\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english',max_df=5, smooth_idf=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 행렬분해\n",
    "\"\"\" Topic modeling \"\"\"\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svd_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=100,random_state=122)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data['OriginalTweet']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "originalTweet = np.array(data['OriginalTweet'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "\n",
    "originalTweet\n",
    "\n",
    "\n",
    "X = vectorizer.fit_transform(originalTweet)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svd_model.fit(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.shape(svd_model.components_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 트레이닝 하면서 단어집합이 저장됨\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "components_ = svd_model.components_\n",
    "\n",
    "# topic의 수 x 단어의 수\n",
    "components_.shape\n",
    "\n",
    "def get_topics(components_, feature_names, n=5):\n",
    "    for idx ,topic in enumerate(components_):\n",
    "        print(\"Topic %d : \" % (idx+1) , [(feature_names[i],topic[i].round(5)) for i in topic.argsort()[:-n-1:-1]])\n",
    "\n",
    "get_topics(components_,terms)\n",
    "# 그다지 서로 비슷한 토픽이 없는 듯 하다\n",
    "\n",
    "\n",
    "\n",
    "# 감성분류가 필요할 듯 한\n",
    "\n",
    "\n",
    "\n",
    "# lsa 차원 축소를 통해 비슷한 단어가 포함된 문서 분류 -> tfidvectorizer 는 lsa 였네\n",
    "\n",
    "# lda  문서-주제-단어 관계로 문서에 포함된 주제별 확률과 주제별 단어가 있는 확률을 나타낸다\n",
    "# 둘 중 하나를 적용하기 위해 tf-idf 행렬 만들기\n",
    "\n",
    "\n",
    "# feature_extraction\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "l     dddd      a          \"\n",
    "l     s   d     a a         \"\n",
    "l     d    d   aaaaa        \"\n",
    "l     d   d   a     a       \"\n",
    "l___  dddd   a       a      \"\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "data['OriginalTweet']\n",
    "\n",
    "\n",
    "data['OriginalTweet'] = data['OriginalTweet'].apply(lambda x: apply_stopwords(x))\n",
    "#preprocessing\n",
    "data['OriginalTweet'] =  data['OriginalTweet'].apply(lambda x: x.split(' '))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data['OriginalTweet']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# i wouldn't install gensim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create dictionary\n",
    "\"\"\"\n",
    "\n",
    "dict = {}\n",
    "\n",
    "word_list = data['OriginalTweet'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "word_list  =np.array(word_list)\n",
    "word_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data['OriginalTweet'] =  data['OriginalTweet'].apply(lambda x: x.split(' '))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% tokenizing\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word_list = data['OriginalTweet']\n",
    "\n",
    "# fail code\n",
    "# word_list.reshape(,-1) ?\n",
    "# 2-dimension to 1-dim\n",
    "#word_list = sum(word_list, [])  ultimate loop\n",
    "#import itertools\n",
    "#word_list_1dim = list(itertools.chain(*word_list)) 한글자씩 나뉜다\n",
    "word_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2-dimension to 1-dim maybe success\n",
    "word_list_1dim = np.concatenate(word_list).tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word_list_1dim\n",
    "\n",
    "\n",
    "\n",
    "word_dict = {}\n",
    "idx = 0\n",
    "for i in range(len(word_list_1dim)):\n",
    "    if word_list_1dim[i] not in word_dict.keys():\n",
    "        word_dict[word_list_1dim[i]] = idx\n",
    "        idx +=1\n",
    "    else:\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### 감성 분류 ########\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "le = LabelEncoder()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## find category\n",
    "\n",
    "sentiment = le.fit(data['Sentiment'])\n",
    "\n",
    "le.transform(sentiment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(word_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fail code\n",
    "# word_cnt_dict={}\n",
    "# for i in range(len(word_list_1dim)):\n",
    "#     if word_list_1dim[i] in word_cnt_dict.keys():\n",
    "#         word_cnt_dict[word_list_1dim[i]] +=1\n",
    "#     else:\n",
    "#         word_cnt_dict[word_list_1dim[i]] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% word count 고 이거는 ..\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "originalTweet = np.array(data['OriginalTweet'])\n",
    "\n",
    "\n",
    "data[['OriginalTweet','Sentiment']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "\n",
    "data['OriginalTweet'] =  data['OriginalTweet'].apply(lambda x: x.split(' '))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "# 문장 토큰화\n",
    "\n",
    "\n",
    "data = pd.read_csv('C:\\\\Users\\\\tjdal\\\\PycharmProjects\\\\Kaggle\\\\Corona_NLP_train.csv', encoding='latin1')\n",
    "\n",
    "#pre processing\n",
    "data['OriginalTweet'] = data['OriginalTweet'].apply(lambda x: re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '',x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "0        MeNyrbie Phil_Gahan Chrisitv httpstcoiFz9FAn2P...\n1        advice Talk to your neighbours family to excha...\n2        Coronavirus Australia Woolworths to give elder...\n3        My food stock is not the only one which is emp...\n4        Me ready to go at supermarket during the COVID...\n                               ...                        \n41152    Airline pilots offering to stock supermarket s...\n41153    Response to complaint not provided citing COVI...\n41154    You know itÂs getting tough when KameronWilds...\n41155    Is it wrong that the smell of hand sanitizer i...\n41156    TartiiCat Well newused Rift S are going for 70...\nName: OriginalTweet, Length: 41157, dtype: object"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['OriginalTweet']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 단어 빈도수로 사전을 우선 만든다 이후 상위 n개의 단어를 추출해서 사용할 수 있기 때문에\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joncoopertweets', 'I', 'took', 'these', 'pictures', 'today', 'at', 'my', 'home', 'grocery', 'store', 'in', 'Montgomery', 'County', 'MD', 'No', 'flour', 'sugar', 'sweet', 'potatoes', 'potatoes', 'orange', 'juice', 'paper', 'towels', 'or', 'toilet', 'paper', 'Low', 'on', 'meat', 'mac', 'amp', ';', 'cheese', 'coronapocolypse', 'Covid_19', 'panicbuying']\n",
      "['MeNyrbie', 'Phil_Gahan', 'Chrisitv', 'httpstcoiFz9FAn2Pa', 'and', 'httpstcoxX6ghGFzCC', 'and', 'httpstcoI2NlzdxNo8']\n",
      "['advice', 'Talk', 'to', 'your', 'neighbours', 'family', 'to', 'exchange', 'phone', 'numbers', 'create', 'contact', 'list', 'with', 'phone', 'numbers', 'of', 'neighbours', 'schools', 'employer', 'chemist', 'GP', 'set', 'up', 'online', 'shopping', 'accounts', 'if', 'poss', 'adequate', 'supplies', 'of', 'regular', 'meds', 'but', 'not', 'over', 'order']\n",
      "['Coronavirus', 'Australia', 'Woolworths', 'to', 'give', 'elderly', 'disabled', 'dedicated', 'shopping', 'hours', 'amid', 'COVID19', 'outbreak', 'httpstcobInCA9Vp8P']\n",
      "['My', 'food', 'stock', 'is', 'not', 'the', 'only', 'one', 'which', 'is', 'empty', 'PLEASE', 'dont', 'panic', 'THERE', 'WILL', 'BE', 'ENOUGH', 'FOOD', 'FOR', 'EVERYONE', 'if', 'you', 'do', 'not', 'take', 'more', 'than', 'you', 'need', 'Stay', 'calm', 'stay', 'safe', 'COVID19france', 'COVID_19', 'COVID19', 'coronavirus', 'confinement', 'Confinementotal', 'ConfinementGeneral', 'httpstcozrlG0Z520j']\n",
      "['Me', 'ready', 'to', 'go', 'at', 'supermarket', 'during', 'the', 'COVID19', 'outbreak', 'Not', 'because', 'Im', 'paranoid', 'but', 'because', 'my', 'food', 'stock', 'is', 'litteraly', 'empty', 'The', 'coronavirus', 'is', 'a', 'serious', 'thing', 'but', 'please', 'dont', 'panic', 'It', 'causes', 'shortage', 'CoronavirusFrance', 'restezchezvous', 'StayAtHome', 'confinement', 'httpstcousmuaLq72n']\n",
      "['As', 'news', 'of', 'the', 'regionÂ\\x92s', 'first', 'confirmed', 'COVID19', 'case', 'came', 'out', 'of', 'Sullivan', 'County', 'last', 'week', 'people', 'flocked', 'to', 'area', 'stores', 'to', 'purchase', 'cleaning', 'supplies', 'hand', 'sanitizer', 'food', 'toilet', 'paper', 'and', 'other', 'goods', 'Tim_Dodson', 'reports', 'httpstcocfXch7a2lU']\n",
      "['Cashier', 'at', 'grocery', 'store', 'was', 'sharing', 'his', 'insights', 'on', 'Covid_19', 'To', 'prove', 'his', 'credibility', 'he', 'commented', 'Im', 'in', 'Civics', 'class', 'so', 'I', 'know', 'what', 'Im', 'talking', 'about', 'httpstcoieFDNeHgDO']\n",
      "['Was', 'at', 'the', 'supermarket', 'today', 'Didnt', 'buy', 'toilet', 'paper', 'Rebel', 'toiletpapercrisis', 'covid_19', 'httpstcoeVXkQLIdAZ']\n",
      "['Due', 'to', 'COVID19', 'our', 'retail', 'store', 'and', 'classroom', 'in', 'Atlanta', 'will', 'not', 'be', 'open', 'for', 'walkin', 'business', 'or', 'classes', 'for', 'the', 'next', 'two', 'weeks', 'beginning', 'Monday', 'March', '16', 'We', 'will', 'continue', 'to', 'process', 'online', 'and', 'phone', 'orders', 'as', 'normal', 'Thank', 'you', 'for', 'your', 'understanding', 'httpstcokw91zJ5O5i']\n",
      "['For', 'corona', 'preventionwe', 'should', 'stop', 'to', 'buy', 'things', 'with', 'the', 'cash', 'and', 'should', 'use', 'online', 'payment', 'methods', 'because', 'corona', 'can', 'spread', 'through', 'the', 'notes', 'Also', 'we', 'should', 'prefer', 'online', 'shopping', 'from', 'our', 'home', 'Its', 'time', 'to', 'fight', 'against', 'COVID', '19', 'govindia', 'IndiaFightsCorona']\n",
      "['All', 'month', 'there', 'hasnt', 'been', 'crowding', 'in', 'the', 'supermarkets', 'or', 'restaurants', 'however', 'reducing', 'all', 'the', 'hours', 'and', 'closing', 'the', 'malls', 'means', 'everyone', 'is', 'now', 'using', 'the', 'same', 'entrance', 'and', 'dependent', 'on', 'a', 'single', 'supermarket', 'manila', 'lockdown', 'covid2019', 'Philippines', 'httpstcoHxWs9LAnF9']\n",
      "['Due', 'to', 'the', 'Covid19', 'situation', 'we', 'have', 'increased', 'demand', 'for', 'all', 'food', 'products', 'The', 'wait', 'time', 'may', 'be', 'longer', 'for', 'all', 'online', 'orders', 'particularly', 'beef', 'share', 'and', 'freezer', 'packs', 'We', 'thank', 'you', 'for', 'your', 'patience', 'during', 'this', 'time']\n",
      "['horningsea', 'is', 'a', 'caring', 'community', 'LetÂ\\x92s', 'ALL', 'look', 'after', 'the', 'less', 'capable', 'in', 'our', 'village', 'and', 'ensure', 'they', 'stay', 'healthy', 'Bringing', 'shopping', 'to', 'their', 'doors', 'help', 'with', 'online', 'shopping', 'and', 'self', 'isolation', 'if', 'you', 'have', 'symptoms', 'or', 'been', 'exposed', 'to', 'somebody', 'who', 'has', 'httpstcolsGrXXhjhh']\n",
      "['Me', 'I', 'dont', 'need', 'to', 'stock', 'up', 'on', 'food', 'Ill', 'just', 'have', 'Amazon', 'deliver', 'whatever', 'I', 'need', 'CoronaVirus', 'Amazon', 'httpstco8YWaKFjExC']\n",
      "['ADARA', 'Releases', 'COVID19', 'Resource', 'Center', 'for', 'Travel', 'Brands', 'Insights', 'Help', 'Travel', 'Brands', 'Stay', 'UpToDate', 'on', 'Consumer', 'Travel', 'Behavior', 'Trends', 'httpstcoPnA797jDKV', 'httpstcodQox6uSihz']\n",
      "['Lines', 'at', 'the', 'grocery', 'store', 'have', 'been', 'unpredictable', 'but', 'is', 'eating', 'out', 'a', 'safe', 'alternative', 'Find', 'out', 'more', 'about', 'whether', 'you', 'should', 'be', 'avoiding', 'restaurants', 'right', 'now', 'httpstco9idZSis5oQ', 'coronavirus', 'covid19', 'httpstcoZHbh898lf6']\n",
      "['_', '13', '_', 'httpstco51bL8P6vZh']\n",
      "['eyeonthearctic', '16MAR20', 'Russia', 'consumer', 'surveillance', 'watchdog', 'reported', 'case', 'in', 'high', 'Arctic', 'where', 'a', 'man', 'who', 'traveled', 'to', 'Iran', 'has', 'COVID19', 'and', '101', 'are', 'observed', 'httpstco4WnrrK9oKC', 'httpstcold05k5Eyns']\n",
      "['Amazon', 'Glitch', 'Stymies', 'Whole', 'Foods', 'Fresh', 'Grocery', 'Deliveries', 'Â\\x93As', 'COVID19', 'has', 'spread', 'weÂ\\x92ve', 'seen', 'a', 'significant', 'increase', 'in', 'people', 'shopping', 'online', 'for', 'groceriesÂ\\x94', 'a', 'spokeswoman', 'said', 'in', 'a', 'statement', 'Â\\x93Today', 'this', 'resulted', 'in', 'a', 'systems', 'impact', 'affecting', 'our', 'httpstcoTbzZ2MC3b3']\n",
      "['For', 'those', 'who', 'arent', 'struggling', 'please', 'consider', 'donating', 'to', 'a', 'food', 'bank', 'or', 'a', 'nonprofit', 'The', 'demand', 'for', 'these', 'services', 'will', 'increase', 'as', 'COVID19', 'impacts', 'jobs', 'and', 'peoples', 'way', 'of', 'life']\n",
      "['with', '100', 'nations', 'inficted', 'with', 'covid', '19', 'the', 'world', 'must', 'not', 'play', 'fair', 'with', 'china', '100', 'goverments', 'must', 'demand', 'china', 'adopts', 'new', 'guilde', 'lines', 'on', 'food', 'safty', 'the', 'chinese', 'goverment', 'is', 'guilty', 'of', 'being', 'irosponcible', 'with', 'life', 'on', 'a', 'global', 'scale']\n",
      "['httpstcoAVKrR9syff', 'The', 'COVID19', 'coronavirus', 'pandemic', 'is', 'impacting', 'consumer', 'shopping', 'behavior', 'purchase', 'decisions', 'and', 'retail', 'sales', 'according', 'to', 'a', 'First', 'Insight', 'study']\n",
      "['We', 'have', 'AMAZING', 'CHEAP', 'DEALS', 'FOR', 'THE', 'COVID2019', 'going', 'on', 'to', 'help', 'you', 'Trials', 'Monthly', 'Yearly', 'And', 'Resonable', 'Prices', 'Subscriptions', 'Just', 'DM', 'US', 'bestiptv', 'iptv', 'Service', 'Iptv', 'iptvdeals', 'Cheap', 'ipTV', 'Football', 'HD', 'Movies', 'Adult', 'Cinema', 'hotmovies', 'iptvnew', 'iptv2020', 'Adult']\n",
      "['We', 'have', 'AMAZING', 'CHEAP', 'DEALS', 'FOR', 'THE', 'COVID2019', 'going', 'on', 'to', 'help', 'you', 'Trials', 'Monthly', 'Yearly', 'And', 'Resonable', 'Prices', 'Subscriptions', 'Just', 'DM', 'US', 'bestiptv', 'iptv', 'Service', 'Iptv', 'iptvdeals', 'Cheap', 'ipTV', 'Football', 'HD', 'Movies', 'Adult', 'Cinema', 'hotmovies', 'ipTv', 'IPTVLinks', '18Movies']\n",
      "['10DowningStreet', 'grantshapps', 'what', 'is', 'being', 'done', 'to', 'ensure', 'food', 'and', 'other', 'essential', 'products', 'are', 'being', 'restocked', 'at', 'supermarkets', 'and', 'panic', 'buying', 'actively', 'discouraged', 'It', 'can', 'not', 'be', 'left', 'to', 'checkout', 'staff', 'to', 'police', 'the', 'actions', 'of', 'the', 'selfish', 'and', 'profiteer']\n",
      "['UK', 'consumer', 'poll', 'indicates', 'the', 'majority', 'expect', 'covid19s', 'impact', 'to', 'last', '412', 'months', 'at', '12', 'March', 'We', 'expect', 'this', 'to', 'increase', 'at', 'the', 'next', 'tracker', 'See', 'full', 'results', 'of', 'the', 'RetailX', 'Coronavirus', 'Consumer', 'Confidence', 'Tracker', 'here', 'httpstcoK3uJlcjqDB', 'httpstco9G3kgqIXJ8']\n",
      "['In', 'preparation', 'for', 'higher', 'demand', 'and', 'a', 'potential', 'food', 'shortage', 'The', 'Hunger', 'Coalition', 'purchased', '10', 'percent', 'more', 'food', 'and', 'implemented', 'new', 'protocols', 'due', 'to', 'the', 'COVID19', 'coronavirus', 'httpstco5CecYtLnYn']\n",
      "['This', 'morning', 'I', 'tested', 'positive', 'for', 'Covid', '19', 'I', 'feel', 'ok', 'I', 'have', 'no', 'symptoms', 'so', 'far', 'but', 'have', 'been', 'isolated', 'since', 'I', 'found', 'out', 'about', 'my', 'possible', 'exposure', 'to', 'the', 'virus', 'Stay', 'home', 'people', 'and', 'be', 'pragmatic', 'I', 'will', 'keep', 'you', 'updated', 'on', 'how', 'IÂ\\x92m', 'doing', 'No', 'panic', 'httpstcoLg7HVMZglZ']\n",
      "['Do', 'you', 'see', 'malicious', 'price', 'increases', 'in', 'NYC', 'The', 'NYC', 'Department', 'of', 'Consumer', 'and', 'Worker', 'Protection', 'DCWP', 'has', 'set', 'up', 'a', 'page', 'to', 'digitally', 'file', 'a', 'complaint', 'Click', 'here', 'httpstcooEx6Y8mm2K', 'To', 'file', 'a', 'complaint', 'use', 'the', 'wordOvercharge', 'httpstcoMdMmoBttOP', 'COVID19', 'CovidNYC']\n",
      "['7SealsOfTheEnd', 'Soon', 'with', 'dwindling', 'supplies', 'unlawful', 'Panicky', 'people', 'will', 'be', 'breaking', 'into', 'Closed', 'Stores', 'amp', ';', 'Supermarkets', 'to', 'Raid', 'them', 'as', 'they', 'normally', 'do', 'during', 'a', 'Crisis', 'so', 'massive', 'as', 'the', 'Coronavirus', 'StockUpamp', ';', 'LockUp']\n",
      "['There', 'Is', 'of', 'in', 'the', 'Country', 'The', 'more', 'empty', 'shelves', 'people', 'see', 'the', 'more', 'buying', 'ensues', 'the', 'more', 'food', 'is', 'out', 'of', 'stock']\n",
      "['Hole', 'Foods', 'images', 'from', 'the', 'nicest', 'grocery', 'store', 'in', 'one', 'of', 'the', 'richest', 'neighborhoods', 'in', 'the', 'United', 'States', 'httpstcoWnQSoMtkVI', 'BreakingNews', 'Breaking', 'Coronavirus', 'CoronavirusOutbreak', 'COVID19', 'COVID19', 'COVID_19', 'COVID2019', 'Collapse']\n",
      "['Retail', 'store', 'closures', 'could', 'explode', 'because', 'of', 'the', 'coronavirus', 'via', 'CNBC', 'BrickAndMortar', 'httpstcohQrYRNXFhv', 'httpstcog5UZn06gb6']\n",
      "['Coronavirus', 'fun', 'fact', 'if', 'you', 'cough', 'at', 'the', 'grocery', 'store', 'you', 'get', 'the', 'whole', 'aisle', 'to', 'yourself', 'pretty', 'quickly', 'CoronavirusOutbreak', 'coronavirus', 'COVID2019']\n",
      "['Were', 'sorry', 'to', 'say', 'that', 'our', 'FinFabUK', 'event', 'is', 'being', 'cancelled', 'due', 'to', 'Covid19', 'The', 'health', 'and', 'wellbeing', 'of', 'our', 'attendees', 'speakers', 'and', 'staff', 'is', 'our', 'top', 'priority', 'Apologies', 'for', 'any', 'disappointment', 'this', 'may', 'cause', 'All', 'FAQs', 'are', 'answered', 'in', 'the', 'link', 'below', 'httpstcoGDDPTudCvj']\n",
      "['Went', 'to', 'the', 'supermarket', 'yesterday', 'and', 'the', 'toilet', 'paper', 'was', 'gone', 'Has', 'this', 'anything', 'to', 'do', 'with', 'the', 'Corona', 'virus', 'COVID2019']\n",
      "['Yes', 'buy', 'only', 'what', 'you', 'need', 'But', 'whats', 'the', 'point', 'of', 'posting', 'photos', 'of', 'those', 'people', 'in', 'the', 'supermarket', 'with', 'a', 'load', 'of', 'stuff', 'They', 'could', 'be', 'buying', 'for', 'all', 'their', 'elderly', 'parents', 'kids', 'siblings', 'etc', 'who', 'cant', 'buy', 'for', 'themselves', 'Not', 'everything', 'needs', 'to', 'be', 'viral', 'Covid19', 'alr', 'is']\n",
      "['Worried', 'about', 'the', 'impact', 'of', 'the', 'current', 'COVID19', 'pandemic', 'on', 'your', 'finances', 'WeÂ\\x92ve', 'just', 'published', 'some', 'tips', 'to', 'help', 'you', 'manage', 'your', 'money', 'during', 'these', 'challenging', 'times', 'COVID19', 'httpstco3jKK3CqXfQ', 'httpstcoEbEnURmmJS']\n",
      "['my', 'wife', 'works', 'retailamp', ';', 'a', 'customer', 'came', 'in', 'yesterday', 'coughing', 'everywhere', 'saying', 'they', 'have', 'CoVid19', 'They', 'requested', 'a', 'deep', 'clean', 'of', 'the', 'store', 'her', 'company', 'objected', 'to', 'due', 'to', 'cost', 'recommending', 'the', 'team', 'spray', 'disinfectantamp', ';', 'clean', 'themselves', 'were', 'gon', 'na', 'dieget', 'sick', 'due', 'to', 'capitalism']\n",
      "['Now', 'I', 'can', 'go', 'to', 'the', 'supermarket', 'like', 'this', 'without', 'being', 'judged', 'CoronavirusOutbreak', 'COVID2019', 'httpstcokrTCGiUHQS']\n",
      "['Were', 'here', 'to', 'provide', 'a', 'safe', 'shopping', 'experience', 'for', 'our', 'customers', 'and', 'a', 'healthy', 'environment', 'for', 'our', 'associates', 'and', 'community', 'Online', 'orders', 'can', 'be', 'placed', 'here', 'httpstcodCSXHUj3U0', 'jlmco', 'jlmcobrand', 'coronapocolypse', 'coronavirus', 'CoronavirusOutbreak', 'COVID19', 'shoponline', 'httpstcoriNKwskeRS']\n",
      "['Curious', 'do', 'we', 'think', 'retail', 'shoppers', 'will', 'do', 'a', 'lot', 'of', 'online', 'shopping', 'bc', 'theyre', 'home', 'and', 'unable', 'to', 'go', 'out', 'or', 'do', 'we', 'think', 'everyone', 'is', 'too', 'spooked', 'to', 'get', 'that', 'extra', 'pair', 'of', 'shoes', 'economy', 'onlineshopping', 'coronavirus', 'covid19', 'stayhome']\n",
      "['CHECK', 'VIDEO', 'httpstco1ksn9Brl02', 'No', 'food', 'in', 'USA', 'market', 'due', 'to', 'coronavirus', 'panic', 'we', 'gon', 'na', 'die', 'from', 'starvation', 'CoronavirusOutbreak', 'coronavirus', 'houston', 'nofood', 'Notoiletpaper', 'NoHandShakes', 'nohandsanitizer', 'COVID19', 'pandemic', 'totallockdown', 'COVID2019usa', 'walmart', 'httpstcoztN3iMkgpD']\n",
      "['Breaking', 'Story', 'Online', 'clothes', 'shopping', 'rises', 'as', 'people', 'find', 'mysterious', 'white', 'patches', 'forming', 'on', 'clothes', 'QuarantineLife', 'CoronavirusOutbreak', 'coronavirus', 'IMadeThisUp', 'FakeNews', 'httpstco5Z24hptT9M']\n",
      "['This', 'is', 'the', 'line', 'outside', 'Target', 'in', 'as', 'customers', 'wait', 'for', 'the', 'store', 'to', 'open', 'this', 'morning']\n",
      "['South', 'Africans', 'stock', 'up', 'on', 'food', 'basic', 'goods', 'as', 'coronavirus', 'panic', 'hits', 'httpstco6nGNFJmy89', 'CoronaVirusSA', 'Covid_19', 'httpstcopzirO10avf']\n",
      "['Please', 'Share', 'Know', 'someone', 'who', 's', '65', 'Living', 'on', 'their', 'own', 'struggling', 'to', 'get', '2', 'their', 'local', 'supermarket', 'due', 'to', 'issues', 'around', '19', 'We', 're', 'offering', 'FREE', 'deliveries', 'of', 'our', 'healthy', 'soups', 'NATIONWIDE', 'to', 'anyone', '65', 'in', 'need', 'Plus', 'their', 'freezable']\n",
      "['People', 'posting', 'and', 'sharing', 'photos', 'of', 'of', 'half', 'to', 'completely', 'empty', 'shelves', 'calling', 'those', 'people', 'dumb', 'or', 'idiots', 'All', 'while', 'shopping', 'at', 'the', 'grocery', 'store', 'lol', 'coronavirus', 'COVID19']\n",
      "['Never', 'thought', 'Id', 'say', 'this', 'but', '2019', 'Will', 'you', 'come', 'back', 'PLEASE', 'coronavirus', 'COVID19', 'peoplearelosingtheirminds', 'StopTheMadness', 'stoppanicbuying']\n",
      "['COVID19', 'restrictions', 'sparking', 'a', 'run', 'on', 'cannabis', 'stores', 'Theyre', 'not', 'closed', 'yet', 'But', 'Customers', 'are', 'stocking', 'up', 'on', 'cannabis', 'this', 'weekend', 'preparing', 'for', 'what', 'could', 'be', 'more', 'retail', 'store', 'restrictions', 'in', 'coming', 'days', 'httpstcoWMqR8QWoiG']\n",
      "['Everything', 'weÂ\\x92re', 'seeing', 'in', 'the', 'current', 'COVID19', 'outbreak', 'has', 'been', 'seen', 'before', 'in', 'previous', 'epidemics', 'and', 'pandemics', ';', 'the', 'rise', 'of', 'fear', 'racism', 'panic', 'buying', 'of', 'food', 'and', 'medicines', 'conspiracy', 'theories', 'the', 'proliferation', 'of', 'quack', 'cures', 'httpstcoPr8NpKX41A']\n",
      "['Everyone', 'is', 'closed', 'but', 'we', 'remain', 'open', 'because', 'we', 'are', 'an', 'emergency', 'store', 'Thank', 'your', 'retail', 'workers', 'covid_19', 'pandemic', 'socialdistancing', 'retail', 'httpstcoWtB0B1AMON']\n",
      "['Why', 'we', 'stock', 'up', 'on', 'water', 'cause', 'utility', 'companies', 'will', 'shut', 'you', 'off', 'in', 'the', 'middle', 'of', 'a', 'pandemic', 'the', 'schools', 'close', 'thier', 'doors', 'you', 'lose', 'out', 'on', 'work', 'cause', 'your', 'kid', 'has', 'no', 'where', 'to', 'go', 'and', 'you', 'canÂ\\x92t', 'afford', 'months', 'worth', 'of', 'food', 'coronavirus', 'SenatorRomney', 'httpstco0CV0793olS']\n",
      "['Dear', 'Coronavirus', 'Ive', 'been', 'following', 'social', 'distancing', 'rules', 'and', 'staying', 'home', 'to', 'prevent', 'the', 'spread', 'of', 'you', 'However', 'now', 'Ive', 'spent', 'an', 'alarming', 'amount', 'of', 'money', 'shopping', 'online', 'Where', 'can', 'I', 'submit', 'my', 'expenses', 'to', 'for', 'reimbursement', 'Let', 'me', 'know', 'coronapocolypse', 'coronavirus']\n",
      "['Global', 'food', 'prices', 'before', 'the', 'spread', 'of', 'COVID', '19', 'intensified', 'across', 'several', 'geographies', 'We', 'could', 'see', 'further', 'downward', 'pressures', 'in', 'the', 'coming', 'months', 'due', 'to', 'continued', 'well', 'supplied', 'markets', 'and', 'the', 'negative', 'impact', 'on', 'demand', 'resulting', 'from', 'the', 'virus']\n",
      "['Morning', 'everyone', 'have', 'a', 'great', 'and', 'safe', 'day', 'coronavirus', 'StopPanicBuying', 'BeKind', 'mufc', 'MUFC_Family']\n",
      "['Of', 'all', 'the', 'things', 'to', 'panic', 'buy', 'in', 'an', 'emergency', 'I', 'dont', 'get', 'why', 'toilet', 'paper', 'is', 'so', 'important', 'If', 'youre', 'afraid', 'of', 'the', 'worst', 'case', 'scenario', 'just', 'wash', 'up', 'in', 'the', 'tub', 'and', 'use', 'your', 'money', 'on', 'food', 'Yall', 'crazy', 'coronavirus']\n",
      "['THANK', 'YOUR', 'GROCERY', 'CLERK', 'Went', 'to', 'grocery', 'store', 'today', 'and', 'looked', 'into', 'the', 'weary', 'eyes', 'of', 'the', 'clerk', 'I', 'thanked', 'her', 'and', 'realized', 'that', 'she', 'was', 'thrust', 'on', 'the', 'front', 'line', 'of', 'this', 'panick', 'A', 'new', 'breed', 'of', 'first', 'responders', 'They', 'are', 'working', 'hard', 'to', 'serve', 'their', 'communities', 'coronavirus']\n",
      "['With', 'the', 'outbreak', 'of', 'Covid19', 'in', 'entire', 'world', 'the', 'retail', 'shops', 'in', 'Malaysia', 'is', 'facing', 'a', 'great', 'challenges', 'In', 'the', 'near', 'future', 'online', 'shopping', 'will', 'be', 'a', 'surprise', 'way', 'for', 'all', 'the', 'people', 'while', 'many', 'will', 'lost', 'their', 'jobs', 'Malaysia2020', 'Malaysia', 'COVID19']\n",
      "['My', 'thoughts', 'on', 'impacts', 'of', 'coronavirus', 'on', 'food', 'markets', 'httpstcobPodDdPRcE']\n",
      "['Consumer', 'Corner', 'Scammers', 'Taking', 'Advantage', 'Of', 'COVID19', 'Fears', 'coronavirus', 'cdc', 'flu', 'trends', 'alert', 'httpstcosk9qCJsnYl', 'httpstcoT7qejP3hys']\n",
      "['4', 'Both', 'the', 'masks', 'made', 'for', 'medical', 'personnel', 'and', 'for', 'consumer', 'purchase', 'require', 'a', 'onceobscure', 'material', 'called', 'meltblown', 'fabric', 'httpstco3hCd9IiWoX']\n",
      "['My', 'work', 'is', 'capitalizing', 'on', 'the', 'demand', 'for', 'packaged', 'food', 'and', 'making', 'us', 'stay', 'open', 'as', 'opposed', 'to', 'closing', 'for', 'all', 'our', 'health', 'and', 'safety', 'LockdownCanada', 'coronavirus']\n",
      "['So', 'are', 'we', 'feeling', 'like', 'its', 'ethical', 'to', 'still', 'do', 'stuff', 'like', 'order', 'deliveries', 'food', 'online', 'shopping', 'etc', 'ship', 'isolation', 'care', 'packages', 'to', 'loved', 'ones', 'etc', 'COVID2019']\n",
      "['What', '2K', 'Consumers', 'Told', 'PYMNTS', 'About', 'How', 'COVID19', 'Changed', 'Their', 'Daily', 'Lives', 'httpstcoYbg8Zupdf6', 'via', 'pymnts']\n",
      "['Bought', 'a', 'house', 'during', 'Covid19', 'panic', 'DidnÂ\\x92t', 'think', 'to', 'buy', 'food', 'for', 'the', 'house', 'Tragic']\n",
      "['Seen', 'in', 'a', 'Facebook', 'group', 'businesses', 'need', 'to', 'stop', 'increasing', 'prices', 'on', 'essentials', 'while', 'we', 'are', 'in', 'an', 'emergency', 'situation', 'itÂ\\x92s', 'frankly', 'despicable', 'and', 'is', 'totally', 'void', 'community', 'spirit', 'nameandshame', 'covid19uk', 'coronavirus', 'Liverpool', 'httpstcoStTAkyqQiZ']\n",
      "['BobJLowe', 'Sadly', 'those', 'are', 'the', 'misinformed', 'thinking', 'that', 'COVID19', 'gives', 'diarrhoea', 'therefore', 'they', 'had', 'to', 'stockpile', 'toilet', 'papers', 'ATM', 'hygiene', 'and', 'food', 'are', 'more', 'important']\n",
      "['TinaMcCauley70', 'Yeah', 'my', 'parents', 'are', 'risky', 'people', 'to', 'the', 'covid', '19', 'thatÂ\\x92s', 'why', 'we', 'stay', 'at', 'home', 'just', 'go', 'to', 'the', 'supermarket', 'when', 'really', 'necessary', 'stay', 'safe', 'too']\n",
      "['CN', 'Coronavirus', 'COVID19', 'I', 'will', 'be', 'in', 'the', 'group', 'and', 'so', 'will', 'my', 'Mum', 'who', 'I', 'live', 'with', 'in', 'the', 'group', 'that', 'needs', 'to', 'be', 'shielded', 'for', '12', 'weeks', '3', 'months', 'This', 'will', 'mean', 'staying', 'in', 'I', 'hope', 'I', 'can', 'still', 'get', 'the', 'online', 'shopping', 'that', 'I', 'need']\n",
      "['ItÂ\\x92s', 'kind', 'of', 'like', 'how', 'saying', 'a', 'word', 'over', 'and', 'over', 'makes', 'it', 'not', 'sound', 'like', 'a', 'word', 'anymore', 'For', 'many', 'of', 'the', 'people', 'who', 'donÂ\\x92t', 'think', 'the', 'COVID19', 'news', 'is', 'BS', 'itÂ\\x92s', 'making', 'them', 'go', 'to', 'the', 'stores', 'and', 'panic', 'buy', 'food', 'and', 'basic', 'necessities', 'until', 'thereÂ\\x92s', 'nothing', 'left']\n",
      "['Hi', 'COVID19', 'Thanks', 'for', 'making', 'me', 'do', 'more', 'online', 'shopping']\n",
      "['Corona', 'scare', 'sends', 'seafood', 'prices', 'skyrocketing', 'in', 'Mumbai', 'gt', ';', 'gt', ';', 'httpstcoGB11EFBYIB', 'seafood', 'coronavirus', 'CoronavirusOutbreak', 'CoronavirusReachesDelhi', 'Coronavid19', 'CoronaVirusUpdates', 'COVID2019', 'COVID19', 'JhalakBollywood', 'JhalakKollywood', 'JhalakTollywood', 'httpstcoU5Dg3LoFYG']\n",
      "['Pausing', 'student', 'loan', 'payments', 'in', 'addition', 'to', 'halting', 'interest', 'accumulation', 'amp', 'stopping', 'punitive', 'student', 'loan', 'collections', 'would', 'provide', 'much', 'needed', 'immediate', 'relief', 'to', 'those', 'individuals', 'unable', 'to', 'work', 'amp', 'are', 'facing', 'economic', 'hardship']\n",
      "['balajis', 'On', 'the', 'consumer', 'side', 'the', 'tech', 'is', 'there', 'some', 'Chinese', 'group', 'already', 'demostrated', 'ELISA', 'test', 'strips', 'for', 'COVID19', 'though', 'details', 'were', 'lacking', 'For', 'consumer', 'though', 'US_FDA', 'would', 'have', 'to', 'deem', 'it', 'as', 'a', 'waived', 'test', 'which', 'doesnÂ\\x92t', 'come', 'that', 'easily']\n",
      "['Lost', 'wages', 'either', 'due', 'to', 'illness', 'from', '19', 'or', 'to', 'the', 'virus', 'economic', 'impact', 'will', 'mean', 'an', 'increased', 'demand', 'We', 'urge', 'and', 'to', 'support', 'a', 'bill', 'that', 'includes', 'support', 'for', 'food', 'banks', 'flexibility', 'for', 'and', 'school', 'meals', 'and', 'increased']\n",
      "['The', 'actions', 'of', 'some', 'are', 'so', 'selfish', 'If', 'I', 'were', 'CEO', 'of', 'a', 'grocery', 'store', 'from', '79', 'am', 'would', 'be', 'a', 'time', 'for', 'people', 'over', '65', 'to', 'shop', ';', 'show', 'ID', 'I', 'just', 'saw', 'a', 'young', 'couple', 'with', '300', 'rolls', 'of', 'tp', 'No', 'one', 'is', 'that', 'full', 'of', 'crap', 'Well', 'maybe', 'CoronavirusOutbreak', 'httpstcoHrbzmh95VQ']\n",
      "['Coronavirus', 'poses', 'a', 'complex', 'puzzle', 'for', 'fooddelivery', 'companies', 'their', 'delivery', 'capacity', 'may', 'buckle', 'under', 'surging', 'demand', 'httpstco1C1cMLmQii', 'via', 'WSJ', 'services', 'food', 'delivery', 'coronavirus']\n",
      "['TheJoshuaTurner', 'Loreign83', 'peanut_astro', 'my_amigouk', 'afneil', 'BorisJohnson', 'patel4witham', 'This', 'is', 'both', 'disgusting', 'and', 'disgraceful', 'charging', 'over', 'inflated', 'prices', 'for', 'items', 'for', 'stopping', 'the', 'spread', 'of', 'COVID19', 'the', 'government', 'really', 'needs', 'to', 'do', 'something', 'abou']\n",
      "['As', 'more', 'retailers', 'close', 'physical', 'stores', 'or', 'curtail', 'hours', 'as', 'a', 'result', 'of', 'Covid19', 'it', 'is', 'agoing', 'to', 'put', 'additional', 'pressure', 'on', 'other', 'omnichannel', 'alternatives', 'like', 'grocery', 'delivery', 'and', 'curbside', 'pick', 'up', 'httpstcokgyDow3Nrz', 'covid19', 'ecommerce', 'omnichannel', 'retail', 'digital']\n",
      "['Check', 'out', 'what', 'these', 'folks', 'are', 'up', 'to', 'here', 'in', 'So', 'Cal', 'I', 'like', 'this', 'idea', 'La', 'Habra', 'supermarket', 'offers', 'special', 'hours', 'for', 'seniors', 'amid', 'COVID19', 'crisis', 'httpstconcTXF8TGyf']\n",
      "['Love', 'it', 'or', 'hate', 'it', 'head', 'advice', '10DowningStreet', 'amp', ';', 'BorisJohnson', 'Blip', 'in', 'our', 'lives', 'but', 'itÂ\\x92s', 'happening', 'DonÂ\\x92t', 'whinge', 'about', 'what', 'you', 'canÂ\\x92t', 'do', 'Dont', 'panic', 'buy', 'as', 'food', 'wont', 'run', 'out', 'DO', 'spend', 'time', 'with', 'the', 'family', 'DO', 'use', 'common', 'sense', 'coronavirus', 'WorldHealthOrg2']\n",
      "['An', 'open', 'letter', 'to', 'consumerdebt', 'holding', 'organizations', 'and', 'others', 'We', 'are', 'at', 'the', 'precipice', 'of', 'a', 'crisis', 'of', 'household', 'economy', 'Please', 'suspend', 'debts', 'and', 'interestfees', 'for', 'sixty', 'days', 'in', 'response', 'to', 'the', 'COVID19', 'crisis', 'Feel', 'free', 'to', 'sign', 'here', 'httpstcoJMZZJOmNT9', 'httpstcoYR2tcPx1Ee']\n",
      "['COVID19', 'COVID19Aus', 'coronavirus', 'Just', 'wanted', 'to', 'spread', 'this', 'news', 'to', 'all', 'older', 'Australians', 'particularly', 'those', 'still', 'mobile', 'but', 'without', 'family', 'support', 'httpstcoyDGX4lK8L0']\n",
      "['Sadly', 'this', 'does', 'not', 'surprise', 'me', 'Heard', 'from', 'one', 'payer', 'exec', 'that', 'they', 'are', 'laying', 'low', 'hoping', 'all', 'blows', 'over', 'Mind', 'bogglingly', 'stupid', 'and', 'this', 'was', 'a', 'nonprofit', 'Blues', 'plan', 'httpstcozr67d1u12Q']\n",
      "['I', 'work', 'in', 'retail', 'I', 'keep', 'stock', 'back', 'for', 'our', 'older', 'customers', 'so', 'when', 'such', 'as', 'frank', 'comes', 'in', 'store', 'for', 'his', 'bread', 'and', 'he', 'sees', 'a', 'empty', 'shelve', 'I', 'say', 'donÂ\\x92t', 'worry', 'pal', 'IÂ\\x92ve', 'saved', 'you', '1', 'Same', 'as', 'pat', 'n', 'her', 'beans', 'Could', 'I', 'get', 'disciplined', 'Yes', 'do', 'I', 'care', 'No', 'IÂ\\x92ve', 'got', 'a', 'coronavirus']\n",
      "['In', 'attempts', 'to', 'lengthen', 'runways', 'marketing', 'budgets', 'are', 'being', 'slashed', 'hiring', 'is', 'being', 'frozen', 'and', 'staffing', 'matrices', 'are', 'being', 'redrawn', 'and', 'dive', 'deep', 'into', 'how', 'consumer', 'startups', 'are', 'battling', 'the', 'impact', 'of', 'on', 'their', 'business']\n",
      "['Â\\x93at', 'this', 'time', 'our', 'distillery', 'remains', 'in', 'operation', 'but', 'we', 'will', 'not', 'be', 'offering', 'public', 'tours', 'or', 'hosting', 'functions', 'or', 'events', 'Our', 'retail', 'store', 'is', 'also', 'closed', 'httpstcolYZg2kfsm0']\n",
      "['Please', 'dont', 'hoard', 'food', 'and', 'water', 'Theres', 'absolutely', 'no', 'need', 'to', 'panic', 'buy', ';', 'the', 'supply', 'chain', 'is', 'completely', 'interrupted', 'And', 'above', 'all', 'please', 'dont', 'hoard', 'sanitizing', 'products', ';', 'there', 'are', 'people', 'out', 'there', 'who', 'really', 'need', 'them', 'probably', 'more', 'than', 'you', 'DontPanicBuy', 'coronavirus']\n",
      "['The', 'fact', 'that', 'canned', 'food', 'toxic', 'chemicals', 'and', 'store', 'bought', 'hand', 'sanitizers', 'are', 'out', 'of', 'stock', 'yet', 'fresh', 'fruit', 'vegetables', 'and', 'herbs', 'are', 'FULLY', 'stocked', 'shows', 'that', 'humans', 'have', 'no', 'idea', 'how', 'the', 'immune', 'system', 'works', 'QuarantineLife', 'COVID2019']\n",
      "['Just', 'called', 'mum', 'and', 'dad', 'in', 'UK', 'over', '70', 'They', 'are', 'great', 'but', 'I', 'offered', 'help', 'with', 'online', 'shopping', 'etc', 'We', 'might', 'sometimes', 'forget', 'that', 'this', 'is', 'not', 'always', 'easy', 'Do', 'the', 'same', 'if', 'you', 'can', 'If', 'you', 'are', 'far', 'from', 'your', 'parents', 'like', 'me', 'Tech', 'can', 'be', 'really', 'useful', 'COVID19', 'Coronavirus']\n",
      "['People', 'seen', 'stocking', 'up', 'on', 'goods', 'into', 'trolleys', 'after', 'the', 'panic', 'buying', 'rumours', 'spread', 'today', 'at', 'hypermarket', 'in', 'Kajang', 'March', '16', '2020', 'Picture', 'by', 'Shafwan', 'Zaidon']\n",
      "['As', 'we', 'often', 'see', 'during', 'major', 'news', 'events', 'criminals', 'will', 'try', 'to', 'take', 'advantage', 'of', 'any', 'situation', 'The', 'Coronavirus', 'or', 'COVID19', 'is', 'no', 'exception', 'Here', 'is', 'some', 'guidance', 'from', 'the', 'Attorney', 'Generals', 'office', 'httpstcoKT5J4QqCwS']\n",
      "['Pretty', 'sure', 'within', 'a', 'week', 'or', 'two', 'supermarket', 'supply', 'chains', 'will', 'dry', 'up', 'as', 'more', 'counties', 'are', 'effected', 'by', 'Covid19', 'and', 'possibly', 'go', 'into', 'lockdown', 'If', 'so', 'would', 'the', 'government', 'introduce', 'a', 'form', 'of', 'rationing', 'so', 'that', 'people', 'can', 'eat', 'Somehow', 'I', 'dont', 'think', 'so']\n",
      "['Supermarket', 'workers', 'are', 'at', 'the', 'frontline', 'of', 'COVID19', 'These', 'are', 'extraordinary', 'times', 'and', 'retail', 'is', 'under', 'extreme', 'pressure', 'When', 'shopping', 'please', 'remain', 'calm', 'and', 'thank', 'the', 'workers', 'that', 'are', 'doing', 'everything', 'they', 'can', 'to', 'keep', 'the', 'shelves', 'stocked', 'and', 'the', 'checkouts', 'moving', 'httpstco0uHGM8gsp8']\n",
      "['Worried', 'about', 'COVID19', 'I', 'more', 'worried', 'about', 'people', 'panicking', 'Having', 'a', 'plan', 'gt', ';', 'Youre', 'not', 'panic', 'buying', 'food', 'gt', ';', 'You', 'can', 'focus', 'on', 'the', 'important', 'issues', 'gt', ';', 'You', 'have', 'the', 'best', 'opportunity', 'for', 'a', 'positive', 'outcome', 'Create', 'structure', 'reduce', 'key', 'decisions', 'Flourish', 'Stay', 'safe']\n",
      "['kroger', 'is', 'the', 'biggest', 'supermarket', 'chain', 'in', 'the', 'United', 'States', 'It', 'has', '453000', 'employees', 'and', 'many', 'receive', 'no', 'sick', 'leave', 'Even', 'after', '2', 'employees', 'tested', 'positive', 'for', 'COVID19', 'kroger', 'still', 'wont', 'provide', 'paid', 'sick', 'leave', 'to', 'everyone', 'httpstco19uNybttHl']\n",
      "['kroger', 'Instead', 'of', 'paid', 'sick', 'leave', 'kroger', 'is', 'providing', '2', 'weeks', 'paid', 'leave', 'ONLY', 'to', 'people', 'who', 'test', 'positive', 'for', 'COVID19', 'or', 'are', 'placed', 'under', 'mandatory', 'quarantine', 'This', 'is', 'insufficient', 'to', 'protect', 'staff', 'and', 'the', 'public', 'especially', 'with', 'little', 'testing', 'av']\n",
      "['I', 'followed', 'this', 'when', 'I', 'went', 'shopping', 'a', 'few', 'days', 'ago', 'Its', 'a', 'pain', 'but', 'necessary', 'Protect', 'Yourself', 'From', 'Grocery', 'Shopping', 'Consumer', 'Reports', 'COVID2019', 'StayHealthy', 'httpstco48nG14me6E']\n"
     ]
    }
   ],
   "source": [
    "word_dict  = {}\n",
    "for i in range(len(data['OriginalTweet'][:100])):\n",
    "    text = sent_tokenize(data['OriginalTweet'][i])\n",
    "    print(sentence)\n",
    "    for j in range(len(text)):\n",
    "        sentence = word_tokenize(text[j])\n",
    "        for word in sentence:\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = 0\n",
    "            else:\n",
    "                word_dict[word] +=1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "['MeNyrbie Phil_Gahan Chrisitv httpstcoiFz9FAn2Pa and httpstcoxX6ghGFzCC and httpstcoI2NlzdxNo8']"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sent_tokenize(data['OriginalTweet'][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_8844/2074010336.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0msentence\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mword_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001B[0m in \u001B[0;36mword_tokenize\u001B[1;34m(text, language, preserve_line)\u001B[0m\n\u001B[0;32m    128\u001B[0m     \u001B[1;33m:\u001B[0m\u001B[0mtype\u001B[0m \u001B[0mpreserve_line\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mbool\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    129\u001B[0m     \"\"\"\n\u001B[1;32m--> 130\u001B[1;33m     \u001B[0msentences\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mpreserve_line\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0msent_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlanguage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    131\u001B[0m     return [\n\u001B[0;32m    132\u001B[0m         \u001B[0mtoken\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0msent\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msentences\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mtoken\u001B[0m \u001B[1;32min\u001B[0m \u001B[0m_treebank_word_tokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msent\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001B[0m in \u001B[0;36msent_tokenize\u001B[1;34m(text, language)\u001B[0m\n\u001B[0;32m    106\u001B[0m     \"\"\"\n\u001B[0;32m    107\u001B[0m     \u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"tokenizers/punkt/{0}.pickle\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlanguage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 108\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    109\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36mtokenize\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1272\u001B[0m         \u001B[0mGiven\u001B[0m \u001B[0ma\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreturns\u001B[0m \u001B[0ma\u001B[0m \u001B[0mlist\u001B[0m \u001B[0mof\u001B[0m \u001B[0mthe\u001B[0m \u001B[0msentences\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mthat\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1273\u001B[0m         \"\"\"\n\u001B[1;32m-> 1274\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msentences_from_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrealign_boundaries\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1275\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1276\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdebug_decisions\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36msentences_from_text\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1326\u001B[0m         \u001B[0mfollows\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mperiod\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1327\u001B[0m         \"\"\"\n\u001B[1;32m-> 1328\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0me\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mspan_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrealign_boundaries\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1329\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1330\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_slices_from_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   1326\u001B[0m         \u001B[0mfollows\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mperiod\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1327\u001B[0m         \"\"\"\n\u001B[1;32m-> 1328\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0me\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mspan_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrealign_boundaries\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1329\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1330\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_slices_from_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36mspan_tokenize\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1316\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mrealign_boundaries\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1317\u001B[0m             \u001B[0mslices\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_realign_boundaries\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mslices\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1318\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0msl\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mslices\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1319\u001B[0m             \u001B[1;32myield\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0msl\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msl\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1320\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36m_realign_boundaries\u001B[1;34m(self, text, slices)\u001B[0m\n\u001B[0;32m   1357\u001B[0m         \"\"\"\n\u001B[0;32m   1358\u001B[0m         \u001B[0mrealign\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1359\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0msl1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msl2\u001B[0m \u001B[1;32min\u001B[0m \u001B[0m_pair_iter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mslices\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1360\u001B[0m             \u001B[0msl1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mslice\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msl1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mrealign\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msl1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1361\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0msl2\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36m_pair_iter\u001B[1;34m(it)\u001B[0m\n\u001B[0;32m    314\u001B[0m     \u001B[0mit\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0miter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    315\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 316\u001B[1;33m         \u001B[0mprev\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    317\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    318\u001B[0m         \u001B[1;32mreturn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36m_slices_from_text\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m   1330\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_slices_from_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1331\u001B[0m         \u001B[0mlast_break\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1332\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0mmatch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lang_vars\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mperiod_context_re\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfinditer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1333\u001B[0m             \u001B[0mcontext\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmatch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgroup\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mmatch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgroup\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"after_tok\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1334\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext_contains_sentbreak\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcontext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "#sentence = word_tokenize(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "vocab_sorted =sorted(word_dict.items(),key=lambda x: x[1], reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "word_to_index  ={}\n",
    "\n",
    "for i, v in enumerate(vocab_sorted):\n",
    "\n",
    "    word_to_index[v[0]] = i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "{'the': 0,\n 'to': 1,\n 'and': 2,\n 'of': 3,\n 'a': 4,\n 'in': 5,\n 'COVID19': 6,\n 'for': 7,\n 'is': 8,\n 'I': 9,\n 'coronavirus': 10,\n 'food': 11,\n 'on': 12,\n 'are': 13,\n 'you': 14,\n 'shopping': 15,\n 'store': 16,\n 'this': 17,\n 'people': 18,\n 'will': 19,\n 'be': 20,\n 'with': 21,\n 'or': 22,\n ';': 23,\n 'that': 24,\n 'online': 25,\n 'panic': 26,\n 'at': 27,\n 'our': 28,\n 'as': 29,\n 'more': 30,\n 'we': 31,\n 'have': 32,\n 'but': 33,\n 'not': 34,\n 'up': 35,\n 'do': 36,\n 'supermarket': 37,\n 'The': 38,\n 'out': 39,\n 'retail': 40,\n 'can': 41,\n 'Coronavirus': 42,\n 'so': 43,\n 'We': 44,\n 'all': 45,\n 'COVID2019': 46,\n 'your': 47,\n 'need': 48,\n 'grocery': 49,\n 'buy': 50,\n 'from': 51,\n 'their': 52,\n 'who': 53,\n 'stock': 54,\n 'demand': 55,\n 'being': 56,\n 'due': 57,\n 'CoronavirusOutbreak': 58,\n 'like': 59,\n 'over': 60,\n 'dont': 61,\n 'go': 62,\n 'my': 63,\n 'about': 64,\n 'spread': 65,\n '19': 66,\n 'Covid19': 67,\n 'has': 68,\n 'consumer': 69,\n 'safe': 70,\n 'during': 71,\n 'toilet': 72,\n 'paper': 73,\n 'what': 74,\n 'home': 75,\n 'time': 76,\n 'been': 77,\n 'they': 78,\n 'impact': 79,\n 'those': 80,\n 'buying': 81,\n 'here': 82,\n 'no': 83,\n 'get': 84,\n 'if': 85,\n 'empty': 86,\n 'stay': 87,\n 'because': 88,\n 'open': 89,\n 'help': 90,\n 'just': 91,\n 'Consumer': 92,\n 'pandemic': 93,\n 'This': 94,\n 'No': 95,\n 'into': 96,\n 'amp': 97,\n 'think': 98,\n 'an': 99,\n 'it': 100,\n 'gt': 101,\n 'hours': 102,\n 'outbreak': 103,\n 'one': 104,\n 'Stay': 105,\n 'please': 106,\n 'news': 107,\n 'stores': 108,\n 'was': 109,\n 'today': 110,\n 'For': 111,\n 'should': 112,\n 'use': 113,\n 'there': 114,\n 'everyone': 115,\n 'these': 116,\n 'Just': 117,\n 'months': 118,\n 'positive': 119,\n 'virus': 120,\n 'how': 121,\n 'see': 122,\n 'could': 123,\n 'They': 124,\n 'etc': 125,\n 'some': 126,\n 'sick': 127,\n 'work': 128,\n 'me': 129,\n 'prices': 130,\n 'If': 131,\n 'still': 132,\n 'group': 133,\n 'really': 134,\n 'would': 135,\n 'kroger': 136,\n 'leave': 137,\n 'family': 138,\n 'phone': 139,\n 'supplies': 140,\n 'My': 141,\n 'FOR': 142,\n 'Im': 143,\n 'It': 144,\n 'As': 145,\n 'case': 146,\n 'purchase': 147,\n 'other': 148,\n 'goods': 149,\n 'his': 150,\n 'Covid_19': 151,\n 'weeks': 152,\n 'March': 153,\n 'orders': 154,\n 'All': 155,\n 'now': 156,\n 'situation': 157,\n 'increased': 158,\n 'products': 159,\n 'may': 160,\n 'community': 161,\n 'after': 162,\n 'healthy': 163,\n 'Amazon': 164,\n 'Travel': 165,\n 'covid19': 166,\n 'seen': 167,\n 'increase': 168,\n 'new': 169,\n 'And': 170,\n 'Adult': 171,\n 'staff': 172,\n 'In': 173,\n 'keep': 174,\n 'them': 175,\n 'shelves': 176,\n 'via': 177,\n 'say': 178,\n 'cause': 179,\n 'parents': 180,\n 'needs': 181,\n 'money': 182,\n 'her': 183,\n 'were': 184,\n 'provide': 185,\n 'customers': 186,\n 'coronapocolypse': 187,\n 'Please': 188,\n '65': 189,\n '2': 190,\n 'while': 191,\n 'closed': 192,\n 'days': 193,\n 'emergency': 194,\n 'workers': 195,\n 'great': 196,\n 'important': 197,\n 'many': 198,\n 'making': 199,\n 'itÂ\\x92s': 200,\n 'when': 201,\n 'test': 202,\n 'support': 203,\n 'delivery': 204,\n 'under': 205,\n 'crisis': 206,\n 'paid': 207,\n 'advice': 208,\n 'neighbours': 209,\n 'numbers': 210,\n 'schools': 211,\n 'set': 212,\n 'order': 213,\n 'elderly': 214,\n 'amid': 215,\n 'only': 216,\n 'which': 217,\n 'PLEASE': 218,\n 'take': 219,\n 'than': 220,\n 'calm': 221,\n 'COVID_19': 222,\n 'confinement': 223,\n 'Me': 224,\n 'Not': 225,\n 'shortage': 226,\n 'first': 227,\n 'came': 228,\n 'County': 229,\n 'last': 230,\n 'week': 231,\n 'hand': 232,\n 'sharing': 233,\n 'To': 234,\n 'he': 235,\n 'know': 236,\n 'covid_19': 237,\n 'Due': 238,\n 'business': 239,\n 'next': 240,\n 'two': 241,\n '16': 242,\n 'Thank': 243,\n 'corona': 244,\n 'stop': 245,\n 'things': 246,\n 'Its': 247,\n 'COVID': 248,\n 'supermarkets': 249,\n 'restaurants': 250,\n 'closing': 251,\n 'same': 252,\n 'lockdown': 253,\n 'wait': 254,\n 'particularly': 255,\n 'thank': 256,\n 'ensure': 257,\n 'doors': 258,\n 'isolation': 259,\n 'symptoms': 260,\n 'Brands': 261,\n '_': 262,\n 'where': 263,\n 'Foods': 264,\n 'Grocery': 265,\n 'struggling': 266,\n 'nonprofit': 267,\n 'services': 268,\n 'impacts': 269,\n 'jobs': 270,\n 'way': 271,\n 'life': 272,\n '100': 273,\n 'covid': 274,\n 'world': 275,\n 'must': 276,\n 'china': 277,\n 'decisions': 278,\n 'AMAZING': 279,\n 'CHEAP': 280,\n 'DEALS': 281,\n 'THE': 282,\n 'going': 283,\n 'Trials': 284,\n 'Monthly': 285,\n 'Yearly': 286,\n 'Resonable': 287,\n 'Prices': 288,\n 'Subscriptions': 289,\n 'DM': 290,\n 'US': 291,\n 'bestiptv': 292,\n 'iptv': 293,\n 'Service': 294,\n 'Iptv': 295,\n 'iptvdeals': 296,\n 'Cheap': 297,\n 'ipTV': 298,\n 'Football': 299,\n 'HD': 300,\n 'Movies': 301,\n 'Cinema': 302,\n 'hotmovies': 303,\n '10DowningStreet': 304,\n 'left': 305,\n 'actions': 306,\n 'selfish': 307,\n 'UK': 308,\n 'expect': 309,\n '12': 310,\n 'full': 311,\n 'morning': 312,\n 'tested': 313,\n 'far': 314,\n 'doing': 315,\n 'Do': 316,\n 'NYC': 317,\n 'file': 318,\n 'complaint': 319,\n 'United': 320,\n 'States': 321,\n 'Breaking': 322,\n 'fact': 323,\n 'Were': 324,\n 'health': 325,\n 'any': 326,\n 'Went': 327,\n 'yesterday': 328,\n 'Corona': 329,\n 'Yes': 330,\n 'But': 331,\n 'posting': 332,\n 'photos': 333,\n 'stuff': 334,\n 'themselves': 335,\n 'everything': 336,\n 'Worried': 337,\n 'current': 338,\n 'times': 339,\n 'works': 340,\n 'saying': 341,\n 'deep': 342,\n 'clean': 343,\n 'gon': 344,\n 'na': 345,\n 'without': 346,\n 'Online': 347,\n 'placed': 348,\n 'unable': 349,\n 'too': 350,\n 'economy': 351,\n 'clothes': 352,\n 'QuarantineLife': 353,\n 'line': 354,\n 'basic': 355,\n 'issues': 356,\n 'offering': 357,\n 'deliveries': 358,\n 'People': 359,\n 'completely': 360,\n 'come': 361,\n 'back': 362,\n 'restrictions': 363,\n 'run': 364,\n 'cannabis': 365,\n 'yet': 366,\n 'stocking': 367,\n 'coming': 368,\n 'before': 369,\n 'remain': 370,\n 'water': 371,\n 'companies': 372,\n 'close': 373,\n 'canÂ\\x92t': 374,\n 'Ive': 375,\n 'staying': 376,\n 'markets': 377,\n 'Of': 378,\n 'why': 379,\n 'Malaysia': 380,\n 'facing': 381,\n 'surprise': 382,\n 'called': 383,\n 'So': 384,\n 'care': 385,\n 'house': 386,\n 'Sadly': 387,\n 'necessary': 388,\n 'mean': 389,\n 'word': 390,\n 'donÂ\\x92t': 391,\n 'seafood': 392,\n 'student': 393,\n 'loan': 394,\n 'stopping': 395,\n 'economic': 396,\n 'though': 397,\n 'BorisJohnson': 398,\n 'government': 399,\n 'pressure': 400,\n 'omnichannel': 401,\n 'idea': 402,\n 'wont': 403,\n 'DO': 404,\n 'older': 405,\n 'plan': 406,\n 'IÂ\\x92ve': 407,\n 'public': 408,\n 'events': 409,\n 'hoard': 410,\n 'supply': 411,\n 'chain': 412,\n 'stocked': 413,\n 'by': 414,\n 'You': 415,\n 'employees': 416,\n 'potatoes': 417,\n 'MeNyrbie': 418,\n 'Phil_Gahan': 419,\n 'Chrisitv': 420,\n 'httpstcoiFz9FAn2Pa': 421,\n 'httpstcoxX6ghGFzCC': 422,\n 'httpstcoI2NlzdxNo8': 423,\n 'Talk': 424,\n 'exchange': 425,\n 'create': 426,\n 'contact': 427,\n 'list': 428,\n 'employer': 429,\n 'chemist': 430,\n 'GP': 431,\n 'accounts': 432,\n 'poss': 433,\n 'adequate': 434,\n 'regular': 435,\n 'meds': 436,\n 'Australia': 437,\n 'Woolworths': 438,\n 'give': 439,\n 'disabled': 440,\n 'dedicated': 441,\n 'httpstcobInCA9Vp8P': 442,\n 'THERE': 443,\n 'WILL': 444,\n 'BE': 445,\n 'ENOUGH': 446,\n 'FOOD': 447,\n 'EVERYONE': 448,\n 'COVID19france': 449,\n 'Confinementotal': 450,\n 'ConfinementGeneral': 451,\n 'httpstcozrlG0Z520j': 452,\n 'ready': 453,\n 'paranoid': 454,\n 'litteraly': 455,\n 'serious': 456,\n 'thing': 457,\n 'causes': 458,\n 'CoronavirusFrance': 459,\n 'restezchezvous': 460,\n 'StayAtHome': 461,\n 'httpstcousmuaLq72n': 462,\n 'regionÂ\\x92s': 463,\n 'confirmed': 464,\n 'Sullivan': 465,\n 'flocked': 466,\n 'area': 467,\n 'cleaning': 468,\n 'sanitizer': 469,\n 'Tim_Dodson': 470,\n 'reports': 471,\n 'httpstcocfXch7a2lU': 472,\n 'Cashier': 473,\n 'insights': 474,\n 'prove': 475,\n 'credibility': 476,\n 'commented': 477,\n 'Civics': 478,\n 'class': 479,\n 'talking': 480,\n 'httpstcoieFDNeHgDO': 481,\n 'Was': 482,\n 'Didnt': 483,\n 'Rebel': 484,\n 'toiletpapercrisis': 485,\n 'httpstcoeVXkQLIdAZ': 486,\n 'classroom': 487,\n 'Atlanta': 488,\n 'walkin': 489,\n 'classes': 490,\n 'beginning': 491,\n 'Monday': 492,\n 'continue': 493,\n 'process': 494,\n 'normal': 495,\n 'understanding': 496,\n 'httpstcokw91zJ5O5i': 497,\n 'preventionwe': 498,\n 'cash': 499,\n 'payment': 500,\n 'methods': 501,\n 'through': 502,\n 'notes': 503,\n 'Also': 504,\n 'prefer': 505,\n 'fight': 506,\n 'against': 507,\n 'govindia': 508,\n 'IndiaFightsCorona': 509,\n 'month': 510,\n 'hasnt': 511,\n 'crowding': 512,\n 'however': 513,\n 'reducing': 514,\n 'malls': 515,\n 'means': 516,\n 'using': 517,\n 'entrance': 518,\n 'dependent': 519,\n 'single': 520,\n 'manila': 521,\n 'covid2019': 522,\n 'Philippines': 523,\n 'httpstcoHxWs9LAnF9': 524,\n 'longer': 525,\n 'beef': 526,\n 'share': 527,\n 'freezer': 528,\n 'packs': 529,\n 'patience': 530,\n 'horningsea': 531,\n 'caring': 532,\n 'LetÂ\\x92s': 533,\n 'ALL': 534,\n 'look': 535,\n 'less': 536,\n 'capable': 537,\n 'village': 538,\n 'Bringing': 539,\n 'self': 540,\n 'exposed': 541,\n 'somebody': 542,\n 'httpstcolsGrXXhjhh': 543,\n 'Ill': 544,\n 'deliver': 545,\n 'whatever': 546,\n 'CoronaVirus': 547,\n 'httpstco8YWaKFjExC': 548,\n 'ADARA': 549,\n 'Releases': 550,\n 'Resource': 551,\n 'Center': 552,\n 'Insights': 553,\n 'Help': 554,\n 'UpToDate': 555,\n 'Behavior': 556,\n 'Trends': 557,\n 'httpstcoPnA797jDKV': 558,\n 'httpstcodQox6uSihz': 559,\n 'Lines': 560,\n 'unpredictable': 561,\n 'eating': 562,\n 'alternative': 563,\n 'Find': 564,\n 'whether': 565,\n 'avoiding': 566,\n 'right': 567,\n 'httpstco9idZSis5oQ': 568,\n 'httpstcoZHbh898lf6': 569,\n '13': 570,\n 'httpstco51bL8P6vZh': 571,\n 'eyeonthearctic': 572,\n '16MAR20': 573,\n 'Russia': 574,\n 'surveillance': 575,\n 'watchdog': 576,\n 'reported': 577,\n 'high': 578,\n 'Arctic': 579,\n 'man': 580,\n 'traveled': 581,\n 'Iran': 582,\n '101': 583,\n 'observed': 584,\n 'httpstco4WnrrK9oKC': 585,\n 'httpstcold05k5Eyns': 586,\n 'Glitch': 587,\n 'Stymies': 588,\n 'Whole': 589,\n 'Fresh': 590,\n 'Deliveries': 591,\n 'Â\\x93As': 592,\n 'weÂ\\x92ve': 593,\n 'significant': 594,\n 'groceriesÂ\\x94': 595,\n 'spokeswoman': 596,\n 'said': 597,\n 'statement': 598,\n 'Â\\x93Today': 599,\n 'resulted': 600,\n 'systems': 601,\n 'affecting': 602,\n 'httpstcoTbzZ2MC3b3': 603,\n 'arent': 604,\n 'consider': 605,\n 'donating': 606,\n 'bank': 607,\n 'peoples': 608,\n 'nations': 609,\n 'inficted': 610,\n 'play': 611,\n 'fair': 612,\n 'goverments': 613,\n 'adopts': 614,\n 'guilde': 615,\n 'lines': 616,\n 'safty': 617,\n 'chinese': 618,\n 'goverment': 619,\n 'guilty': 620,\n 'irosponcible': 621,\n 'global': 622,\n 'scale': 623,\n 'httpstcoAVKrR9syff': 624,\n 'impacting': 625,\n 'behavior': 626,\n 'sales': 627,\n 'according': 628,\n 'First': 629,\n 'Insight': 630,\n 'study': 631,\n 'iptvnew': 632,\n 'iptv2020': 633,\n 'ipTv': 634,\n 'IPTVLinks': 635,\n '18Movies': 636,\n 'grantshapps': 637,\n 'done': 638,\n 'essential': 639,\n 'restocked': 640,\n 'actively': 641,\n 'discouraged': 642,\n 'checkout': 643,\n 'police': 644,\n 'profiteer': 645,\n 'poll': 646,\n 'indicates': 647,\n 'majority': 648,\n 'covid19s': 649,\n '412': 650,\n 'tracker': 651,\n 'See': 652,\n 'results': 653,\n 'RetailX': 654,\n 'Confidence': 655,\n 'Tracker': 656,\n 'httpstcoK3uJlcjqDB': 657,\n 'httpstco9G3kgqIXJ8': 658,\n 'preparation': 659,\n 'higher': 660,\n 'potential': 661,\n 'Hunger': 662,\n 'Coalition': 663,\n 'purchased': 664,\n '10': 665,\n 'percent': 666,\n 'implemented': 667,\n 'protocols': 668,\n 'httpstco5CecYtLnYn': 669,\n 'Covid': 670,\n 'feel': 671,\n 'ok': 672,\n 'isolated': 673,\n 'since': 674,\n 'found': 675,\n 'possible': 676,\n 'exposure': 677,\n 'pragmatic': 678,\n 'updated': 679,\n 'IÂ\\x92m': 680,\n 'httpstcoLg7HVMZglZ': 681,\n 'malicious': 682,\n 'price': 683,\n 'increases': 684,\n 'Department': 685,\n 'Worker': 686,\n 'Protection': 687,\n 'DCWP': 688,\n 'page': 689,\n 'digitally': 690,\n 'Click': 691,\n 'httpstcooEx6Y8mm2K': 692,\n 'wordOvercharge': 693,\n 'httpstcoMdMmoBttOP': 694,\n 'CovidNYC': 695,\n '7SealsOfTheEnd': 696,\n 'Soon': 697,\n 'dwindling': 698,\n 'unlawful': 699,\n 'Panicky': 700,\n 'breaking': 701,\n 'Closed': 702,\n 'Stores': 703,\n 'Supermarkets': 704,\n 'Raid': 705,\n 'normally': 706,\n 'Crisis': 707,\n 'massive': 708,\n 'StockUpamp': 709,\n 'LockUp': 710,\n 'There': 711,\n 'Is': 712,\n 'Country': 713,\n 'ensues': 714,\n 'Hole': 715,\n 'images': 716,\n 'nicest': 717,\n 'richest': 718,\n 'neighborhoods': 719,\n 'httpstcoWnQSoMtkVI': 720,\n 'BreakingNews': 721,\n 'Collapse': 722,\n 'Retail': 723,\n 'closures': 724,\n 'explode': 725,\n 'CNBC': 726,\n 'BrickAndMortar': 727,\n 'httpstcohQrYRNXFhv': 728,\n 'httpstcog5UZn06gb6': 729,\n 'fun': 730,\n 'cough': 731,\n 'whole': 732,\n 'aisle': 733,\n 'yourself': 734,\n 'pretty': 735,\n 'quickly': 736,\n 'sorry': 737,\n 'FinFabUK': 738,\n 'event': 739,\n 'cancelled': 740,\n 'wellbeing': 741,\n 'attendees': 742,\n 'speakers': 743,\n 'top': 744,\n 'priority': 745,\n 'Apologies': 746,\n 'disappointment': 747,\n 'FAQs': 748,\n 'answered': 749,\n 'link': 750,\n 'below': 751,\n 'httpstcoGDDPTudCvj': 752,\n 'gone': 753,\n 'Has': 754,\n 'anything': 755,\n 'whats': 756,\n 'point': 757,\n 'load': 758,\n 'kids': 759,\n 'siblings': 760,\n 'cant': 761,\n 'viral': 762,\n 'alr': 763,\n 'finances': 764,\n 'WeÂ\\x92ve': 765,\n 'published': 766,\n 'tips': 767,\n 'manage': 768,\n 'challenging': 769,\n 'httpstco3jKK3CqXfQ': 770,\n 'httpstcoEbEnURmmJS': 771,\n 'wife': 772,\n 'retailamp': 773,\n 'customer': 774,\n 'coughing': 775,\n 'everywhere': 776,\n 'CoVid19': 777,\n 'requested': 778,\n 'company': 779,\n 'objected': 780,\n 'cost': 781,\n 'recommending': 782,\n 'team': 783,\n 'spray': 784,\n 'disinfectantamp': 785,\n 'dieget': 786,\n 'capitalism': 787,\n 'Now': 788,\n 'judged': 789,\n 'httpstcokrTCGiUHQS': 790,\n 'experience': 791,\n 'environment': 792,\n 'associates': 793,\n 'httpstcodCSXHUj3U0': 794,\n 'jlmco': 795,\n 'jlmcobrand': 796,\n 'shoponline': 797,\n 'httpstcoriNKwskeRS': 798,\n 'Curious': 799,\n 'shoppers': 800,\n 'lot': 801,\n 'bc': 802,\n 'theyre': 803,\n 'spooked': 804,\n 'extra': 805,\n 'pair': 806,\n 'shoes': 807,\n 'onlineshopping': 808,\n 'stayhome': 809,\n 'CHECK': 810,\n 'VIDEO': 811,\n 'httpstco1ksn9Brl02': 812,\n 'USA': 813,\n 'market': 814,\n 'die': 815,\n 'starvation': 816,\n 'houston': 817,\n 'nofood': 818,\n 'Notoiletpaper': 819,\n 'NoHandShakes': 820,\n 'nohandsanitizer': 821,\n 'totallockdown': 822,\n 'COVID2019usa': 823,\n 'walmart': 824,\n 'httpstcoztN3iMkgpD': 825,\n 'Story': 826,\n 'rises': 827,\n 'find': 828,\n 'mysterious': 829,\n 'white': 830,\n 'patches': 831,\n 'forming': 832,\n 'IMadeThisUp': 833,\n 'FakeNews': 834,\n 'httpstco5Z24hptT9M': 835,\n 'outside': 836,\n 'Target': 837,\n 'South': 838,\n 'Africans': 839,\n 'hits': 840,\n 'httpstco6nGNFJmy89': 841,\n 'CoronaVirusSA': 842,\n 'httpstcopzirO10avf': 843,\n 'Share': 844,\n 'Know': 845,\n 'someone': 846,\n 's': 847,\n 'Living': 848,\n 'own': 849,\n 'local': 850,\n 'around': 851,\n 're': 852,\n 'FREE': 853,\n 'soups': 854,\n 'NATIONWIDE': 855,\n 'anyone': 856,\n 'Plus': 857,\n 'freezable': 858,\n 'half': 859,\n 'calling': 860,\n 'dumb': 861,\n 'idiots': 862,\n 'lol': 863,\n 'Never': 864,\n 'thought': 865,\n 'Id': 866,\n '2019': 867,\n 'Will': 868,\n 'peoplearelosingtheirminds': 869,\n 'StopTheMadness': 870,\n 'stoppanicbuying': 871,\n 'sparking': 872,\n 'Theyre': 873,\n 'Customers': 874,\n 'weekend': 875,\n 'preparing': 876,\n 'httpstcoWMqR8QWoiG': 877,\n 'Everything': 878,\n 'weÂ\\x92re': 879,\n 'seeing': 880,\n 'previous': 881,\n 'epidemics': 882,\n 'pandemics': 883,\n 'rise': 884,\n 'fear': 885,\n 'racism': 886,\n 'medicines': 887,\n 'conspiracy': 888,\n 'theories': 889,\n 'proliferation': 890,\n 'quack': 891,\n 'cures': 892,\n 'httpstcoPr8NpKX41A': 893,\n 'Everyone': 894,\n 'socialdistancing': 895,\n 'httpstcoWtB0B1AMON': 896,\n 'Why': 897,\n 'utility': 898,\n 'shut': 899,\n 'off': 900,\n 'middle': 901,\n 'thier': 902,\n 'lose': 903,\n 'kid': 904,\n 'afford': 905,\n 'worth': 906,\n 'SenatorRomney': 907,\n 'httpstco0CV0793olS': 908,\n 'Dear': 909,\n 'following': 910,\n 'social': 911,\n 'distancing': 912,\n 'rules': 913,\n 'prevent': 914,\n 'However': 915,\n 'spent': 916,\n 'alarming': 917,\n 'amount': 918,\n 'Where': 919,\n 'submit': 920,\n 'expenses': 921,\n 'reimbursement': 922,\n 'Let': 923,\n 'Global': 924,\n 'intensified': 925,\n 'across': 926,\n 'several': 927,\n 'geographies': 928,\n 'further': 929,\n 'downward': 930,\n 'pressures': 931,\n 'continued': 932,\n 'well': 933,\n 'supplied': 934,\n 'negative': 935,\n 'resulting': 936,\n 'Morning': 937,\n 'day': 938,\n 'StopPanicBuying': 939,\n 'BeKind': 940,\n 'mufc': 941,\n 'MUFC_Family': 942,\n 'youre': 943,\n 'afraid': 944,\n 'worst': 945,\n 'scenario': 946,\n 'wash': 947,\n 'tub': 948,\n 'Yall': 949,\n 'crazy': 950,\n 'THANK': 951,\n 'YOUR': 952,\n 'GROCERY': 953,\n 'CLERK': 954,\n 'looked': 955,\n 'weary': 956,\n 'eyes': 957,\n 'clerk': 958,\n 'thanked': 959,\n 'realized': 960,\n 'she': 961,\n 'thrust': 962,\n 'front': 963,\n 'panick': 964,\n 'A': 965,\n 'breed': 966,\n 'responders': 967,\n 'working': 968,\n 'hard': 969,\n 'serve': 970,\n 'communities': 971,\n 'With': 972,\n 'entire': 973,\n 'shops': 974,\n 'challenges': 975,\n 'near': 976,\n 'future': 977,\n 'lost': 978,\n 'Malaysia2020': 979,\n 'thoughts': 980,\n 'httpstcobPodDdPRcE': 981,\n 'Corner': 982,\n 'Scammers': 983,\n 'Taking': 984,\n 'Advantage': 985,\n 'Fears': 986,\n 'cdc': 987,\n 'flu': 988,\n 'trends': 989,\n 'alert': 990,\n 'httpstcosk9qCJsnYl': 991,\n 'httpstcoT7qejP3hys': 992,\n '4': 993,\n 'Both': 994,\n 'masks': 995,\n 'made': 996,\n 'medical': 997,\n 'personnel': 998,\n 'require': 999,\n ...}"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word ={}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i,v in enumerate(word_to_index)\n",
    "\n",
    "#data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "0        MeNyrbie Phil_Gahan Chrisitv tcoiFz9FAn2Pa and...\n1        advice Talk to your neighbours family to excha...\n2        Coronavirus Australia Woolworths to give elder...\n3        My food stock is not the only one which is emp...\n4        Me ready to go at supermarket during the COVID...\n                               ...                        \n41152    Airline pilots offering to stock supermarket s...\n41153    Response to complaint not provided citing COVI...\n41154    You know itÂs getting tough when KameronWilds...\n41155    Is it wrong that the smell of hand sanitizer i...\n41156    TartiiCat Well newused Rift S are going for 70...\nName: OriginalTweet, Length: 41157, dtype: object"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_8844/429324837.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mword_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001B[0m in \u001B[0;36mword_tokenize\u001B[1;34m(text, language, preserve_line)\u001B[0m\n\u001B[0;32m    128\u001B[0m     \u001B[1;33m:\u001B[0m\u001B[0mtype\u001B[0m \u001B[0mpreserve_line\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mbool\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    129\u001B[0m     \"\"\"\n\u001B[1;32m--> 130\u001B[1;33m     \u001B[0msentences\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mpreserve_line\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0msent_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlanguage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    131\u001B[0m     return [\n\u001B[0;32m    132\u001B[0m         \u001B[0mtoken\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0msent\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msentences\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mtoken\u001B[0m \u001B[1;32min\u001B[0m \u001B[0m_treebank_word_tokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msent\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001B[0m in \u001B[0;36msent_tokenize\u001B[1;34m(text, language)\u001B[0m\n\u001B[0;32m    106\u001B[0m     \"\"\"\n\u001B[0;32m    107\u001B[0m     \u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"tokenizers/punkt/{0}.pickle\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlanguage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 108\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    109\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36mtokenize\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1272\u001B[0m         \u001B[0mGiven\u001B[0m \u001B[0ma\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreturns\u001B[0m \u001B[0ma\u001B[0m \u001B[0mlist\u001B[0m \u001B[0mof\u001B[0m \u001B[0mthe\u001B[0m \u001B[0msentences\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mthat\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1273\u001B[0m         \"\"\"\n\u001B[1;32m-> 1274\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msentences_from_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrealign_boundaries\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1275\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1276\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdebug_decisions\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36msentences_from_text\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1326\u001B[0m         \u001B[0mfollows\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mperiod\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1327\u001B[0m         \"\"\"\n\u001B[1;32m-> 1328\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0me\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mspan_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrealign_boundaries\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1329\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1330\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_slices_from_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   1326\u001B[0m         \u001B[0mfollows\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mperiod\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1327\u001B[0m         \"\"\"\n\u001B[1;32m-> 1328\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0me\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mspan_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrealign_boundaries\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1329\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1330\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_slices_from_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36mspan_tokenize\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1316\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mrealign_boundaries\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1317\u001B[0m             \u001B[0mslices\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_realign_boundaries\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mslices\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1318\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0msl\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mslices\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1319\u001B[0m             \u001B[1;32myield\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0msl\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msl\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1320\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36m_realign_boundaries\u001B[1;34m(self, text, slices)\u001B[0m\n\u001B[0;32m   1357\u001B[0m         \"\"\"\n\u001B[0;32m   1358\u001B[0m         \u001B[0mrealign\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1359\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0msl1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msl2\u001B[0m \u001B[1;32min\u001B[0m \u001B[0m_pair_iter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mslices\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1360\u001B[0m             \u001B[0msl1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mslice\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msl1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mrealign\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msl1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1361\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0msl2\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36m_pair_iter\u001B[1;34m(it)\u001B[0m\n\u001B[0;32m    314\u001B[0m     \u001B[0mit\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0miter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    315\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 316\u001B[1;33m         \u001B[0mprev\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    317\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    318\u001B[0m         \u001B[1;32mreturn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tjdal\\pycharmprojects\\kaggle\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36m_slices_from_text\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m   1330\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_slices_from_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1331\u001B[0m         \u001B[0mlast_break\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1332\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0mmatch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lang_vars\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mperiod_context_re\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfinditer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1333\u001B[0m             \u001B[0mcontext\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmatch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgroup\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mmatch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgroup\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"after_tok\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1334\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext_contains_sentbreak\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcontext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "word_tokenize(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "['@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8']"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "data['OriginalTweet'] = data['OriginalTweet'].apply(lambda x: ' '.join(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "       UserName  ScreenName                      Location     TweetAt  \\\n0          3799       48751                        London  16-03-2020   \n1          3800       48752                            UK  16-03-2020   \n2          3801       48753                     Vagabonds  16-03-2020   \n3          3802       48754                           NaN  16-03-2020   \n4          3803       48755                           NaN  16-03-2020   \n...         ...         ...                           ...         ...   \n41152     44951       89903  Wellington City, New Zealand  14-04-2020   \n41153     44952       89904                           NaN  14-04-2020   \n41154     44953       89905                           NaN  14-04-2020   \n41155     44954       89906                           NaN  14-04-2020   \n41156     44955       89907  i love you so much || he/him  14-04-2020   \n\n                                           OriginalTweet           Sentiment  \n0      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n1      advice Talk to your neighbours family to excha...            Positive  \n2      Coronavirus Australia: Woolworths to give elde...            Positive  \n3      My food stock is not the only one which is emp...            Positive  \n4      Me, ready to go at supermarket during the #COV...  Extremely Negative  \n...                                                  ...                 ...  \n41152  Airline pilots offering to stock supermarket s...             Neutral  \n41153  Response to complaint not provided citing COVI...  Extremely Negative  \n41154  You know itÂs getting tough when @KameronWild...            Positive  \n41155  Is it wrong that the smell of hand sanitizer i...             Neutral  \n41156  @TartiiCat Well new/used Rift S are going for ...            Negative  \n\n[41157 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3799</td>\n      <td>48751</td>\n      <td>London</td>\n      <td>16-03-2020</td>\n      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3800</td>\n      <td>48752</td>\n      <td>UK</td>\n      <td>16-03-2020</td>\n      <td>advice Talk to your neighbours family to excha...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3801</td>\n      <td>48753</td>\n      <td>Vagabonds</td>\n      <td>16-03-2020</td>\n      <td>Coronavirus Australia: Woolworths to give elde...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3802</td>\n      <td>48754</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>My food stock is not the only one which is emp...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3803</td>\n      <td>48755</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>Me, ready to go at supermarket during the #COV...</td>\n      <td>Extremely Negative</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>41152</th>\n      <td>44951</td>\n      <td>89903</td>\n      <td>Wellington City, New Zealand</td>\n      <td>14-04-2020</td>\n      <td>Airline pilots offering to stock supermarket s...</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>41153</th>\n      <td>44952</td>\n      <td>89904</td>\n      <td>NaN</td>\n      <td>14-04-2020</td>\n      <td>Response to complaint not provided citing COVI...</td>\n      <td>Extremely Negative</td>\n    </tr>\n    <tr>\n      <th>41154</th>\n      <td>44953</td>\n      <td>89905</td>\n      <td>NaN</td>\n      <td>14-04-2020</td>\n      <td>You know itÂs getting tough when @KameronWild...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>41155</th>\n      <td>44954</td>\n      <td>89906</td>\n      <td>NaN</td>\n      <td>14-04-2020</td>\n      <td>Is it wrong that the smell of hand sanitizer i...</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>41156</th>\n      <td>44955</td>\n      <td>89907</td>\n      <td>i love you so much || he/him</td>\n      <td>14-04-2020</td>\n      <td>@TartiiCat Well new/used Rift S are going for ...</td>\n      <td>Negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>41157 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_text,val_text,train_label,val_label=train_test_split(train.OriginalTweet,train.Sentiment,\n",
    "                                                             test_size=0.3,random_state=42)\n",
    "# 모델 트레이닝 제출할 것은 아니므로 100문장 ,속도의 우려\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "corpus = [(word_cnt_dict[text],word_dict[text]) for text in originalTweet]\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data\n",
    "\n",
    "# LDA LSA는 나의 연습용이고 데이터를 좀만 보니 감성분류이다\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CNN-LSTM 적용\n",
    "\n",
    "\n",
    "\n",
    "## 다른 사람 (kaggle) 풀이 참고\n",
    "# 이제와서 새삼 EDA 단어 분포\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize': (11,4)})\n",
    "sns.countplot(data['Sentiment'])\n",
    "\n",
    "# 감정을 3가지로 줄여보겠다는 idea\n",
    "#\n",
    "# def change_sen(sentiment):\n",
    "#     if sentiment == \"Extremely Positive\":\n",
    "#         return 'positive'\n",
    "#     elif sentiment == \"Extremely Negative\":\n",
    "#         return 'negative'\n",
    "#     elif sentiment == \"Positive\":\n",
    "#         return 'positive'\n",
    "#     elif sentiment == \"Negative\":\n",
    "#         return 'negative'\n",
    "#     else:\n",
    "#         return 'netural'\n",
    "\n",
    "#Cleaning the tweet\n",
    "#\n",
    "# def clean(text):\n",
    "#     #     remove urls\n",
    "#     text = re.sub(r'http\\S+', \" \", text)\n",
    "#     #     remove mentions\n",
    "#     text = re.sub(r'@\\w+',' ',text)\n",
    "#     #     remove hastags\n",
    "#     text = re.sub(r'#\\w+', ' ', text)\n",
    "#     #     remove digits\n",
    "#     text = re.sub(r'\\d+', ' ', text)\n",
    "#     #     remove html tags\n",
    "#     text = re.sub('r<.*?>',' ', text)\n",
    "#     #     remove stop words\n",
    "#     text = text.split()\n",
    "#     text = \" \".join([word for word in text if not word in stop_word])\n",
    "#\n",
    "#     return text\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}