{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n \n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-15T12:57:42.530990Z","iopub.execute_input":"2022-05-15T12:57:42.531426Z","iopub.status.idle":"2022-05-15T12:57:42.540410Z","shell.execute_reply.started":"2022-05-15T12:57:42.531377Z","shell.execute_reply":"2022-05-15T12:57:42.539167Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"path = \"../input/h-and-m_personalized-fashion-recommendations/\"","metadata":{"execution":{"iopub.status.busy":"2022-05-15T12:58:09.483916Z","iopub.execute_input":"2022-05-15T12:58:09.484816Z","iopub.status.idle":"2022-05-15T12:58:09.489171Z","shell.execute_reply.started":"2022-05-15T12:58:09.484764Z","shell.execute_reply":"2022-05-15T12:58:09.488496Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport gc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\nimport datetime\nimport itertools\nimport os\nfrom contextlib import redirect_stdout\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-05-15T12:57:42.690519Z","iopub.execute_input":"2022-05-15T12:57:42.691912Z","iopub.status.idle":"2022-05-15T12:57:42.703892Z","shell.execute_reply.started":"2022-05-15T12:57:42.691835Z","shell.execute_reply":"2022-05-15T12:57:42.703017Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"rand = 64\nlgb_params = {\n    'objective' : \"binary\",\n    \"boosting\" : \"gbdt\",\n    \"max_depth\" : -1,\n    \"num_leaves\" : 40,\n    \"subsample\" : 0.8 ,\n    \"subsample_freq\" : 1,\n    \"bagging_seed\" : rand,\n    \"learning_rate\" : 0.05,\n    \"feature_fraction\" : 0.6,\n    \"min_data_in_leaf\":100,\n    \"lambda_l1\": 0,\n    \"lambda_l2\" : 0,\n    \"random_state\" : rand,\n    \"metric\" : \"auc\", #binary_logloss\n    \"verbose\" : -1\n        \n}","metadata":{"execution":{"iopub.status.busy":"2022-05-15T12:57:43.463999Z","iopub.execute_input":"2022-05-15T12:57:43.464760Z","iopub.status.idle":"2022-05-15T12:57:43.471411Z","shell.execute_reply.started":"2022-05-15T12:57:43.464715Z","shell.execute_reply":"2022-05-15T12:57:43.470087Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"tran_dtypes = {\"t_dat\" : \"str\", \n               \"customer_id\"  : \"str\",\n              \"article_id\" : \"int\" ,\n              \"product_code\" : \"int\",\n              \"price\" : \"float\", \n              \"sales_channel_id\" : \"int\"}\nart_dtypes ={\"article_id\" : \"int\", \n              \"product_code\" : \"int\" ,\n             \"product_type_no\" : \"int\",\n              \"graphical_apparance_no\" : \"int\", \n             \"color_group_code\" : \"int\" ,\n             \"department_no\" : \"int\",\n            \"index_code\" :\"str\",\n            \"index_group_no\" : \"int\",\n            \"section_no\" : \"int\", \n            \"garment_group_no\" : \"int\"}\ncust_dtypes ={\"customer_id\" : \"str\"}","metadata":{"execution":{"iopub.status.busy":"2022-05-15T12:57:43.985304Z","iopub.execute_input":"2022-05-15T12:57:43.985952Z","iopub.status.idle":"2022-05-15T12:57:43.992234Z","shell.execute_reply.started":"2022-05-15T12:57:43.985909Z","shell.execute_reply":"2022-05-15T12:57:43.991306Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"obj = \"class\"\nN = 15000\nn_iter = 2\nidx_file =\"exp05\"\nlen_hist = 366\nn_round = 2000\nn_splits = 1\ntr_set = [1,8,15,22] #set of train date\nlen_tr = 7 # length of validation period\nnobuy = 20 # num of negative samples","metadata":{"execution":{"iopub.status.busy":"2022-05-15T12:57:44.357397Z","iopub.execute_input":"2022-05-15T12:57:44.357732Z","iopub.status.idle":"2022-05-15T12:57:44.364407Z","shell.execute_reply.started":"2022-05-15T12:57:44.357694Z","shell.execute_reply":"2022-05-15T12:57:44.362927Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cos_sim(v1,v2):\n    return np.dot(v1,v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T12:12:23.913845Z","iopub.execute_input":"2022-05-15T12:12:23.915208Z","iopub.status.idle":"2022-05-15T12:12:23.920725Z","shell.execute_reply.started":"2022-05-15T12:12:23.915144Z","shell.execute_reply":"2022-05-15T12:12:23.919965Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\ndef read_data(day_oldest):\n    # read csv 시 type 읽기\n    df_art = pd.read_csv(path+\"articles.csv\" , dtype =art_dtypes)\n    le = LabelEncoder()\n    le.fit(df_art[\"index_code\"].unique())\n    df_art[\"index_code\"] = le.transform(df_art[\"index_code\"])\n    df_trans = pd.read_csv(path+\"transactions_train.csv\" ,dtype=tran_dtypes)\n    df_trans[\"t_dat\"] = pd.to_datetime(df_trans[\"t_dat\"] , format=\"%Y-%m-%d\")\n    \n    df_trans = df_trans.query(f\"t_dat >= '{day_oldest}'\").copy()\n    df_trans = df_trans.drop_duplicates([\"customer_id\",  \"article_id\" , \"t_dat\"])\n    df_trans = df_trans.merge(df_art[[\"article_id\",\"product_code\", \"product_type_no\" , \"graphical-appearance_no\" , \"color_group_code\", \"department_no\"  ,\"index_code\" , \"index_group_no\", \"section_no\" , \"garment_group_no\"]],how=\"left\" ,on=\"article_id\")\n    \n    df_cust = pd.read_csv(path+\"customers.csv\",dtype=cust_dtypes)\n    df_cust[\"age\"] = df_cust[\"age\"].fillna(df_cust[\"age\"].mean())\n    df_cust[[\"FN\",\"Active\"]] = df_cust[[\"FN\",\"Active\"]].fillna(0)\n    df_cust[\"club_member_status\"] = df_cust[\"club_member_status\"].apply(lambda x:1 if x == \"ACTIVE\" else 0)\n    df_cust[\"fashion_news_frequency\"] = df_cust[\"fashion_news_frequency\"].apply(lambda x:0 if x ==  \"NONE\" else 1)\n    \n    \n    dict_vec = {}\n    vec_art = np.load(\"../input/h-m-rapids-article2vec/articles.npy\")\n    df_vec = pd.concat([df_art[\"article_id\"], pd.DataFrame(vec_art)] , axis=1)\n    for i in range(len(vec_art)):\n        dict_vec[df_art[\"article_id\"][i]] = vec_art[i]\n    del vec_art , df_vec\n    \n    return df_trans, df_art, df_cust , dict_vec","metadata":{"execution":{"iopub.status.busy":"2022-05-15T12:58:41.838481Z","iopub.execute_input":"2022-05-15T12:58:41.838801Z","iopub.status.idle":"2022-05-15T12:58:41.856031Z","shell.execute_reply.started":"2022-05-15T12:58:41.838769Z","shell.execute_reply":"2022-05-15T12:58:41.854945Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def feat_store(df_trans,l_cust,ds,de,dsr,der,dsh,deh):\n    feat = {}\n    \n    df_trans_yesterday = df_trans.query(\"(t_dat == @der)\")\n    df_trans_recent = df_trans.query(\"(t_dat >= @dsr) and (t_dat <= @der)\" )\n    df_trans_hist = df_trans.query(\"(t_dat >= @dsh) and (t_dat <= @deh)\")\n    \n    \n    feat[\"art_buy_hist\"] = df_trans_hist.groupby([\"article_id\"])[\"t_dat\"].agg(art_buy_hist=\"count\")\n    feat[\"art_buy_recent\"] = df_trans_recent.groupby([\"article_id\"])[\"t_dat\"].agg(art_buy_recent=\"count\")\n    feat[\"art_buy_yesterday\"] = df_trans_yesterday.groupby([\"article_id\"])[\"t_dat\"].agee(art_buy_yesterday=\"count\")\n    df_buy1 = df_trans_hist.groupby(\"article_id\")[\"customer_id\"].nuniuqe().reset_index().rename(columns={\"customer_id\" : \"cnt_buy1\"})\n    df_buy2=  df_trans_hist[df_trans_hist.duplicated([\"customer_id\", \"article_id\"])].copy()\n    df_buy2 =  df_buy2.drop_duplicates([\"customer_id\",\"article_id\"])\n    df_buy2 = df_buy2.groupby(\"article_id\")[\"article_id\"].agg(cny_buy2=\"count\").reset_index()\n    df_buy = pd.merge(df_buy1,df_buy2, how=\"left\" , on=\"article_id\").fillna(0)\n    df_buy[\"rebuy_rate\"] = df_buy[\"cnt_buy2\"]/ df_buy[\"cnt_buy1\"]\n    feat[\"rebuy_rate\"] = df_buy[[\"article_id\", \"rebuy_rate\"]]\n    \n    df_trans_yesterday = df_trans_yesterday.query(\"(customer_id in @l_cust)\")\n    df_trans_recent = df_trans_recent.query(\"(customer_id in @l_cust)\")\n    df_trans_hist  = df_trans_hist.query(\"(customer_id in @l_cust)\")\n    feat[\"rate_sales_channel_hist\"] = df_trans_hist.groupby([\"customer_id\"])[\"sales_channel_id\"].agg(rate_sales_channel_hist=\"mean\")\n    feat[\"rate_sales_channel_recent\"] = df_trans_recent.groupby([\"customer_id\"])[\"sale_channel_id\"].agg(rate_sales_channel_recent=\"mean\")\n    \n    feat[\"n_buy_hist\"] = df_trans_hist.groupby([\"customer_id\",\"article_id\"])[\"t_dat\"].agg(n_buy_hist=\"count\")\n    feat[\"n_buy_recent\"] = df_trans_recent.groupby([\"customer_id\", \"article_id\"])[\"t_dat\"].agg(n_buy_recent=\"count\")\n    feat[\"days_after_buy\"] = df_trans_hist.groupby([\"customer_id\", \"article_id\"])[\"t_dat\"].agg(days_after_buy=lambda x: (ds- max(x)).days)\n    \n    feat[\"n_buy_hist_all\"] = df_trans_hist.groupby([\"customer_id\"])[\"t_dat\"].agg(n_buy_hist_all=\"count\")\n    feat[\"n_buy_recent_all\"] = df_trans_recent.groupby([\"customer_id\"])[\"t_dat\"].agg(n_buy_recent_all=\"count\")\n    feat[\"days_after_buy_all\"] = df_trans_hist.groupby([\"customer_id\"])[\"t_dat\"].agg(days_after_buy_all=lambda x: (ds-max(x)).days)\n    feat[\"n_buy_hist_prod\"] = df_trans_hist.groupby([\"customer_id\",\"product_code\"])[\"t_dat\"].agg(n_buy_hist_prod=\"count\")\n    feat[\"n_buy_recent_prod\"] = df_trans_recent.groupby([\"customer_id\", \"product_code\"])[\"t_dat\"].agg(n_buy_recent_prod=\"count\")\n    feat[\"days_after_buy_prod\"] = df_trans_hist.groupby([\"customer_id\", \"product_code\"])[\"t_dat\"].agg(days_after_buy_prod=lambda x:(ds - max(x)).days)\n    feat[\"n_buy_hist_ptype\"] = df_trans_hist.groupby([\"cutsomer\",\"product_type_no\"])[\"t_dat\"].agg(n_buy_hist_ptype=\"count\")\n    feat[\"n_buy_recent_ptype\"] = df_trans_recent.groupby([\"customer_id\",\"product_type_no\"])[\"t_dat\"].agg(n_buy_recent_ptype=\"count\")\n    feat[\"days_after_buy_ptype\"] = df_trans_hist.groupby([\"customer_id\", \"product_type_no\"])[\"t_dat\"].agg(days_after_buy_ptype=lambda x: (ds-max(x)).days)\n    \n    feat[\"n_buy_hist_graph\"] = df_trans_hist.groupby([\"customer_id\", \"graphical_appearance_no\"])[\"t_dat\"].agg(n_buy_hist_graph=\"count\")\n    feat[\"n_buy_recent_graph\"] = df_trans_recent.groupby([\"customer_id\" , \"graphical_appearance_no\"])[\"t_dat\"].agg(n_buy_recent_graph=\"count\")\n    feat[\"days_after_buy_graph\"] = df_trans_hist.groupby([\"customer_id\",\"graphical_appearance_no\"])[\"t_dat\"].agg(days_after_buy_graph=lambda x:(ds-max(x)).days)\n    \n    feat[\"n_buy_hist_dep\"] = df_trans_hist.gropuby([\"customer_id\",\"department_no\"])[\"t_dat\"].agg(n_buy_hist_dep=\"count\")\n    feat[\"n_buy_recent_dep\"] = df_trans_recent.groupby([\"customer_id\", \"department_no\"])[\"t_dat\"].agg(n_buy_recent_dep=\"count\")\n    feat[\"days_after_buy_dep\"] = df_trans_hist.groupby([\"customer_id\", \"department_no\"])[\"t_dat\"].agg(days_after_buy_dep=lambda x: (ds - max(x)).days)\n    \n    feat[\"n_buy_hist_idx\"] = df_trans_hist.groupby([\"customer_id\",\"index_code\"])[\"t_dat\"].agg(n_buy_hist_idx=\"count\")\n    feat[\"n_buy_recent_idx\"] = df_trans_recent.groupby([\"customer_id\",\"index_code\"])[\"t_dat\"].agg(n_buy_recent_idx=\"count\")\n    feat[\"days_after_buy_idx\"] = df_trans_hist.groupby([\"customer_id\",\"index_code\"] )[\"t_dat\"].agg(days_after_buy_dep=lambda x: (ds-max(x)).days)\n    feat[\"n_buy_hist_idxg\"] = df_trans_hist.groupby([\"customer_id\",\"index_group_no\"])[\"t_dat\"].agg(n_buy_hist_idxg=\"count\")\n    feat[\"n_buy_recent_idxg\"] = df_trans_recent.groupby([\"customer_id\",\"index_group_no\"])[\"t_dat\"].agg(days_after_buy_idxg=lambda x: (ds-max(x)).days)\n    \n    feat[\"days_after_buy_idxg\"] = df_trans_hist.groupby([\"customer_id\",\"index_group_no\"])[\"t_dat\"].agg(n_buy_recent_idxg=\"count\")\n    feat[\"n_buy_hist_sec\"] = df_trans_hist.groupby([\"customer_id\",\"section_no\"])[\"t_dat\"].agg(n_buy_hist_sec=\"count\")\n    feat[\"n_buy_recent_sec\"]  = df_trans_recent.groupby([\"customer_id\",\"section_no\"])[\"t_dat\"].agg(n_buy_recent_sec=\"count\")\n    \n    feat[\"days_after_buy_sec\"]= df_trans_hist.groupby([\"customer_id\",\"section_no\"])[\"t_dat\"].agg(days_after_buy_sec=lambda x: (ds-max(x)).days)\n    \n    feat[\"n_buy_hist_garm\"] = df_trans_hist.groupby([\"customer_id\",\"garment_group_no\"])[\"t_dat\"].agg(n_buy_hist_garm=\"count\")\n    feat[\"n_buys_recent_garm\"] = df_trans_recent.groupby(['customer_id',\"garment_group_no\"])[\"t_dat\"].agg(n_buy_recent_garm=\"count\")\n    feat[\"days_after_buy_garm\"] = df_trans_hist.groupby([\"customer_id\"]).groupby([\"article_id\"])[\"article_id\"].apply(list).rename(\"art_id_recent\")\n    \n    del df_trans_yesterday, df_trans_recent, df_trans_hist , df_buy1, df_buy2, df_buy\n    \n    gc.collect()\n    \n    return feat","metadata":{"execution":{"iopub.status.busy":"2022-05-15T12:12:29.475697Z","iopub.execute_input":"2022-05-15T12:12:29.476086Z","iopub.status.idle":"2022-05-15T12:12:29.513982Z","shell.execute_reply.started":"2022-05-15T12:12:29.476048Z","shell.execute_reply":"2022-05-15T12:12:29.512809Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def add_feat(df,ds,de,dsr,der, dsh,deh , feat,dict_vec):\n\n    \n    #left_on / right_on : 열기준 병합 시 기준으로 할 열의 양측 이름이 다르다면, 각각 어떤 열을 기준으로 할지 정해줍니다.\n    #열의 이름을 입력하면 됩니다.\n    \n    \n    # rate_sales_channel_hist\n    df = df.merge(feat[\"rate_sales_channel_hist\"], how=\"left\" , left_on=[\"customer_id\"], right_index=True)\n    # rate_sales_channel_recent\n    df = df.merge(feat[\"rate_sales_channel_recent\"], how=\"left\", left_on=[\"customer_id\"], right_index=True)\n    #art buy_hist\n    \n    df = df.merge(feat[\"art_buy_hist\"], how=\"left\", left_on = [\"article_id\"], right_index=True)\n    # art_buy_recent\n    df = df.merge(feat[\"art_buy_recent\"], how=\"left\" , left_on=[\"article_id\"], right_index=True)\n    # art_buy_yesterday\n    df = df.merge(feat[\"art_buy_yesterday\"], how=\"left\",left_on=[\"article_id\"], right_index=True)\n    # n_buy_hist\n    df = df.merge(feat[\"n_buy_hist\"], how=\"left\", left_on = [\"customer_id\",\"article_id\"], right_index=True)\n    \n    #n_buy_recent\n    df = df.merge(feat[\"n_buy_recent\"], how=\"left\" , left_on= [\"customer_id\", \"article_id\"],right_index=True)\n    # days_after_buy\n    df = df.merge(feat[\"days_after_buy\"] , how=\"left\" , left_on =[\"customer_id\"], right_index=True)\n    \n    # n_buy_hist_all\n    df = df.merge(feat[\"n_buy_hist_all\"] , how=\"left\", left_on=[\"customer_id\"] , right_index=True)\n    \n    # n_buy_recent_all\n    df = df.merge(feat[\"n_buy_recent_all\"], how=\"left\" , left_on=[\"customer_id\"], right_index=True)\n    \n    \n    # days_after_buy_all\n    df = df.merge(feat[\"days_after_buy_all\"],how=\"left\", left_on=[\"customer_id\"], right_index=True)\n    \n    # n_buy_hist_prod\n    \n    df = df.merge(feat[\"n_buy_hist_prod\"], how=\"left\", left_on=[\"customer_id\",\"product_code\"] ,right_index=True)\n    # n_buy_recent_prod\n    df = df.merge(feat[\"n_buy_recent_prod\"],how=\"left\", left_on=[\"customer_id\",\"product_code\"],right_index=True)\n    \n    #days_after_buy_ptype\n    df = df.merge(feat[\"days_after_buy_ptype\"],how=\"left\",left_on=[\"customer_id\",\"product_type_no\"],right_index=True)\n    \n    # n_buy_hist_graph\n    df = df.merge(feat[\"n_buy_hist_graph\"], how=\"left\" , left_on=[\"customer_id\",\"graphical_appearance_no\"],right_index=True)\n    \n    # n_buy_recent_graph\n    df = df.merge(feat[\"days_after_buy_graph\"], how=\"left\" , left_on=[\"customer_id\",\"graphical_appearance_no\"],right_index=True)\n    \n    # days_after_buy_graph\n    df = df.merge(feat[\"days_after_buy_graph\"], how=\"left\",left_on=[\"customer_id\",\"graphical_appearance_no\"],right_index=True)\n    \n    # n_buy_hist_col\n    df = df.merge(feat[\"n_buy_histy_col\"], how=\"left\", left_on=[\"customer_id\",\"colour_group_code\"], right_index=True)\n    \n    \n    \n    # n_buy_recent_col\n    df = df.merge(feat[\"n_buy_recent_col\"], how=\"left\", left_on=[\"customer_id\",\"colour_group_code\"],right_index=True)\n    \n    # days_after_buy_col\n    # left join 이므로 왼쪽의 키가 하나만 있어도 ()\n    df = df.merge(feat[\"days_after_buy_col\"],how=\"left\", left_on=[\"customer_id\",\"colour_group_code\"])\n    \n    #how : 병합시 기준이 될 인덱스를 정하는 방식입니다. left는 기존객체,\n    #   left_index / right_index : 인덱스 기준 병합 시 True로 하면 해당 객체의 인덱스가 병합 기준이됩니다.\n    #※ 즉 left_on을 입력하고 right_index를 True로 한다면 열-인덱스 기준 병합도 가능합니다.\n    \n    # n_buy_hist_dep\n    df = df.merge(feat[\"n_buy_hist_dep\"] , how=\"left\", left_on=[\"customer_id\", \"colour_group_code\"], right_index=True)\n    \n      # n_buy_recent_dep\n    df = df.merge(feat[\"n_buy_recent_dep\"],how=\"left\",left_on=[\"customer_id\",\"department_no\"], right_index=True)\n    # days_after_buy_dep\n    df = df.merge(feat[\"days_after_buy_dep\"],how=\"left\",left_on=[\"customer_id\",\"department_no\"], right_index=True)\n    # n_buy_hist_idx\n    df = df.merge(feat[\"n_buy_hist_idx\"],how=\"left\",left_on=[\"customer_id\",\"index_code\"], right_index=True)\n    # n_buy_recent_idx\n    df = df.merge(feat[\"n_buy_recent_idx\"],how=\"left\",left_on=[\"customer_id\",\"index_code\"], right_index=True)\n    # days_after_buy_idx\n    df = df.merge(feat[\"days_after_buy_idx\"],how=\"left\",left_on=[\"customer_id\",\"index_code\"], right_index=True)\n    # n_buy_hist_idxg\n    df = df.merge(feat[\"n_buy_hist_idxg\"],how=\"left\",left_on=[\"customer_id\",\"index_group_no\"], right_index=True)\n    # n_buy_recent_idxg\n    df = df.merge(feat[\"n_buy_recent_idxg\"],how=\"left\",left_on=[\"customer_id\",\"index_group_no\"], right_index=True)\n    # days_after_buy_idxg\n    df = df.merge(feat[\"days_after_buy_idxg\"],how=\"left\",left_on=[\"customer_id\",\"index_group_no\"], right_index=True)\n    # n_buy_hist_sec\n    df = df.merge(feat[\"n_buy_hist_sec\"],how=\"left\",left_on=[\"customer_id\",\"section_no\"], right_index=True)\n    # n_buy_recent_sec\n    df = df.merge(feat[\"n_buy_recent_sec\"],how=\"left\",left_on=[\"customer_id\",\"section_no\"], right_index=True)\n    # days_after_buy_sec\n    df = df.merge(feat[\"days_after_buy_sec\"],how=\"left\",left_on=[\"customer_id\",\"section_no\"], right_index=True)\n    # n_buy_hist_garm\n    df = df.merge(feat[\"n_buy_hist_garm\"],how=\"left\",left_on=[\"customer_id\",\"garment_group_no\"], right_index=True)\n    # n_buy_recent_garm\n    df = df.merge(feat[\"n_buy_recent_garm\"],how=\"left\",left_on=[\"customer_id\",\"garment_group_no\"], right_index=True)\n    # days_after_buy_garm\n    df = df.merge(feat[\"days_after_buy_garm\"],how=\"left\",left_on=[\"customer_id\",\"garment_group_no\"], right_index=True)\n    # rebuy_rate\n    df = df.merge(feat[\"rebuy_rate\"],how=\"left\",on=\"article_id\")\n    # sim_article\n    df = df.merge(feat[\"art_id_recent\"],how=\"left\",left_on=\"customer_id\", right_index = True)\n    \n    sim_max, sim_sum , sim_mean = [],[],[]\n    \n    # dsiplay(df[[\"article_id\", \"art_id_recent\"]].head())\n    \n    tmp = df[[\"article_id\", \"art_id_recent\"]].values\n    for i in range(len(df)):\n        if not isinstance(tmp[i][1],list):\n            sim_max.append(0);sim_sum.append(0);sim_mean.append(0)\n        else:\n            list_sim = [cos_sim(dict_vec[tmp[i][0]],dict_vec[x]) for x in tmp[i][1]]\n            sim_max.appned(max(list_sim))\n            sim_sum.append(sum(list_sum))\n            sim_mean.append(np.mean(list_sim))\n    df[\"sim_max\"] = sim_max\n    df[\"sim_sum\"] = sim_sum\n    df[\"sim_mean\"] = sim_mean\n    df = df.drop([\"art_id_recent\"], axis=1)\n\n    # fillna\n    df[[\"n_buy_hist\",\"n_buy_recent\",\"n_buy_hist_all\",\"n_buy_recent_all\",\"n_buy_hist_prod\",\"n_buy_recent_prod\",\"n_buy_hist_ptype\",\"n_buy_recent_ptype\",\"n_buy_hist_graph\",\"n_buy_recent_graph\",\n      \"n_buy_hist_col\",\"n_buy_recent_col\",\"n_buy_hist_dep\",\"n_buy_recent_dep\",\"n_buy_hist_idx\",\"n_buy_recent_idx\",\"n_buy_hist_idxg\",\"n_buy_recent_idxg\",\"n_buy_hist_sec\",\"n_buy_recent_sec\",\n      \"n_buy_hist_garm\",\"n_buy_recent_garm\",\"art_buy_yesterday\",\"art_buy_recent\",\"art_buy_hist\",\"rebuy_rate\", \"sim_max\", \"sim_sum\", \"sim_mean\"]] =\\\n    df[[\"n_buy_hist\",\"n_buy_recent\",\"n_buy_hist_all\",\"n_buy_recent_all\",\"n_buy_hist_prod\",\"n_buy_recent_prod\",\"n_buy_hist_ptype\",\"n_buy_recent_ptype\",\"n_buy_hist_graph\",\"n_buy_recent_graph\",\n      \"n_buy_hist_col\",\"n_buy_recent_col\",\"n_buy_hist_dep\",\"n_buy_recent_dep\",\"n_buy_hist_idx\",\"n_buy_recent_idx\",\"n_buy_hist_idxg\",\"n_buy_recent_idxg\",\"n_buy_hist_sec\",\"n_buy_recent_sec\",\n      \"n_buy_hist_garm\",\"n_buy_recent_garm\",\"art_buy_yesterday\",\"art_buy_recent\",\"art_buy_hist\",\"rebuy_rate\", \"sim_max\", \"sim_sum\", \"sim_mean\"]].fillna(0)\n\n    df[[\"days_after_buy\",\"days_after_buy_all\",\"days_after_buy_prod\",\"days_after_buy_ptype\",\"days_after_buy_graph\",\"days_after_buy_col\",\"days_after_buy_dep\",\"days_after_buy_idx\",\n      \"days_after_buy_idxg\",\"days_after_buy_sec\",\"days_after_buy_garm\"]] = \\\n    df[[\"days_after_buy\",\"days_after_buy_all\",\"days_after_buy_prod\",\"days_after_buy_ptype\",\"days_after_buy_graph\",\"days_after_buy_col\",\"days_after_buy_dep\",\"days_after_buy_idx\",\n      \"days_after_buy_idxg\",\"days_after_buy_sec\",\"days_after_buy_garm\"]].fillna(10+len_hist)\n    \n    df[[\"rate_sales_channel_hist\",\"rate_sales_channel_recent\"]] = df[[\"rate_sales_channel_hist\", \"rate_sales_channel_recent\"]].fillna(1.5)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-15T12:12:31.106818Z","iopub.execute_input":"2022-05-15T12:12:31.107205Z","iopub.status.idle":"2022-05-15T12:12:31.155883Z","shell.execute_reply.started":"2022-05-15T12:12:31.107169Z","shell.execute_reply":"2022-05-15T12:12:31.154318Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def recommend_train(day_start_val):\n    day_start = [day_start_val - datetime.timedelta(days=i-1+len_tr) for  i in tr_set]\n    day_end = [day_start_val - datetime.timedelta(days=i) for i in tr_set]\n    day_start_rec = [ x - datetime.timedelta(days=7) for x in day_start]\n    day_end_rec = [x - datetime.timedelta(days=1) for x in day_start]\n    day_start_hist = [x - datetime.timedelta(days=len_hist) for x in day_start]\n    day_end_hist = [x - datetime.timedelta(days=1) for x in day_start]\n    days_start_rec_test = day_start_val - datetime.timedelta(days=7)\n    day_end_rec_test = day_start_val - datetime.timedelta(days=1)\n    day_start_hist_test = day_start_val  - datetime.timedelta(days=1+len_hist)\n    day_end_hist_test = day_start_val - datetime.timedelta(days=1)\n    day_end_val = day_start_val + datetime.timedelta(days=6)\n    \n    df_trans, df_art , df_cust , dict_vec = read_data(day_oldest = day_start_hist[-1])\n    \n    q_date = \"\"\n    \n    for i in range(len(day_start)):\n        if i ==0: q_date = f\"((t_dat>= '{day_start[0]}') and (t_dat <= '{day_end[0]}'))\"\n        else: q_date = q_date  + f\" or ((t_dat >= '{day_start[i]}') and (t_dat <= '{day_end[i]}'))\"\n    top_art_all = df_trans.query(q_date).groupby(\"article_id\")[\"t_dat\"].count().sort_values(ascending=False ).index[:N].tolist()\n    \n    list_df_buy = []\n    list_list_cust = []\n    \n    # make positive samples\n    for i in range(len(day_start)):\n        list_df_buy.append(df_trans.query(f\"(t_dat >= '{day_start[i]}') and ( t_dat <= '{day_end[i]}') and (article_id in @top_art_all)\").drop_duplicates([\"customer_id\",\"article_id\"])[[\"customer_id\",\"article_id\"]].copy())\n        list_df_buy[i][\"target\"] = 1\n        list_list_cust.append(list_df_buy[i][\"customer_id\"].unique().tolist())\n    # make negative samples(random pick)\n    \n    for iter_train in tqdm(range(n_iter)):\n        list_df_nobuy = []\n        list_train = []\n        for i in range(len(day_start)):\n            list_df_nobuy.append(pd.concat([pd.DataFrame({\"customer_id\": x, \"article_id\": random.sample(top_art_all,nobuy)}) for x in list_list_cust[i]]))\n            list_df_nobuy[i][\"target\"] = 0\n            list_train.append(pd.concat([list_df_buy[i], list_df_nobuy[i]]).drop_duplicates([\"customer_id\",\"article_id\"]))\n        del list_df_nobuy\n        display(list_train[0][\"target\"].value_counts())\n        \n        # add feature\n        df_train = pd.DataFrame()\n        for i in tqdm(range(len(day_start))):\n            feat = feat_store(df_trans, list_list_cust[i],day_start[i],day_end[i] , day_start_rec[i],day_end_rec[i], day_start_hist[i], day_end_hist[i])\n            list_train[i]= list_train[i].merge(df_art[[\"article_id\", \"product_code\",\"product_type_no\",\"graphical_appearance_no\", \"colour_group_code\",\"department_no\",\"index_code\",\"index_group_no\",\"section_no\",\"garment_group_no\"]], how=\"left\", on=\"article_id\")\n            list_train[i] = list_train[i].merge(df_cust[[\"customer_id\",\"age\",\"FN\",\"Active\",\"club_member_status\",\"fashion_news_frequency\"]],how=\"left\", on=\"customer_id\")\n            \n            df_train = df_train.append(add_feat(list_train[i],day_start[i],day_end[i],day_start_rec[i],day_end_rec[i],day_start_hist[i],day_end_hist[i],feat,dict_vec))\n            for i in tqdm(range(len(day_start))):\n                feat = feat_store(df_trans,list_list_cust[i],day_start[i], day_end[i],day_start_rec[i],day_end_rec[i],day_start_hist[i],day_end_hist[i])\n                \n                list_train[i] = list_train[i].merge(df_art[[\"article_id\", \"product_code\", \"product_type_no\", \"graphical_appearance_no\",\"colour_group_code\"\n                                                           , \"department_no\", \"index_code\" ,\"index_group_no\" , \"section_no\",\"garment_group_no\"]], how=\"left\",on=\"article_id\")\n                list_train[i] = list_train[i].merge(df_cust[[\"customer_id\",\"age\",\"FN\" ,\"Active\", \"club_member_status\", \"fashion_news_frequency\"]],how=\"left\",on=\"customer_id\")\n                df_train = df_train.append(add_feat(list_train[i],day_start[i], day_end[i], day_start_rec[i],day_end_rec[i] , day_start_hist[i],day_end_hist[i] ,feat,dict_vec))\n                del feat\n            del list_train\n            gc.collect()\n            \n            # train lgbm\n            X_train = df_train.drop([\"customer_id\", \"product_code\", \"product_type_no\", \"department_no\",\"target\"],axis=1)\n            y_train = df_train[\"target\"]\n            del df_train\n            \n            list_model = []\n            if n_splits == 1:\n                X_tr, X_va , y_tr, y_va = train_test_split(X_train, y_train , stratify = y_train)\n                d_tr = lgb.Dataset(X_tr, label=y_tr, free_raw_data=False)\n                d_va = lgb.Dataset(X_va , label=y_va, free_raw_data=False)\n                list_model.append(lgb.train(lgb_params,train_set = d_tr,num_boost_round= n_round , valid_sets= [d_tr,d_va], verbose_eval=500,early_stopping_rounds=100))\n            else:\n                folds = StratifiedKFold(n_splits = n_splits , shuffle = True , random_state = rand)\n                for tr_idx, va_idx in folds.split(X_train, y_train):\n                    X_tr, X_va, y_tr, y_va = X_train.iloc[tr_idx] , X_train.iloc[va_idx], y_train.iloc[tr_idx], y_train.iloc[va_idx]\n                    d_tr = lgb.Dataset(X_tr, label=y_tr,free_raw_data=False)\n                    d_va = lgb.Dataset(X_va, label=y_va, free_raw_data=False)\n                    list_model.append(lgb.train(lgb_params, train_set=d_tr , num_boost_round=n_round, valid_sets=[d_tr,d_va],verbose_eval=500, early_stopping_rounds=100))\n                # save model\n                pd.to_pickle(list_model, f\"models_{iter_train}.pkl\")\n                del X_train , y_train, X_tr, X_va , y_tr, y_va , d_tr, d_va\n                gc.collect()\n            del df_trans, df_art , df_cust\n            gc.collect()\n            return 0\n        \n    \n                \n            ","metadata":{"execution":{"iopub.status.busy":"2022-05-15T12:56:42.963112Z","iopub.execute_input":"2022-05-15T12:56:42.963408Z","iopub.status.idle":"2022-05-15T12:56:42.999060Z","shell.execute_reply.started":"2022-05-15T12:56:42.963373Z","shell.execute_reply":"2022-05-15T12:56:42.998195Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"recommend_train(datetime.datetime(2020,9,23))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T12:58:43.602621Z","iopub.execute_input":"2022-05-15T12:58:43.603851Z","iopub.status.idle":"2022-05-15T12:58:43.666017Z","shell.execute_reply.started":"2022-05-15T12:58:43.603798Z","shell.execute_reply":"2022-05-15T12:58:43.664180Z"},"trusted":true},"execution_count":32,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/953708933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrecommend_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2020\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_33/2337184114.py\u001b[0m in \u001b[0;36mrecommend_train\u001b[0;34m(day_start_val)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mday_end_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mday_start_val\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdf_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_art\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdf_cust\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdict_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mday_oldest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mday_start_hist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mq_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_33/2349958761.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(day_oldest)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mday_oldest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# read csv 시 type 읽기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf_art\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"articles.csv\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mart_dtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_art\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"index_code\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/h-and-m_personalized-fashion-recommendations/articles.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/h-and-m_personalized-fashion-recommendations/articles.csv'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}